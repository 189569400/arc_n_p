10.3 A Design Checklist for Testability 可测试性设计检查表
===

Table 10.2 is a checklist to support the design and analysis process for testability.

TABLE 10.2 Checklist to Support the Design and Analysis Process for Testability

Category | Checklist
---|---
Allocation of Responsibilities | Determine which system responsibilities are most critical and hence need to be most thoroughly tested. <br>Ensure that additional system responsibilities have been allocated to do the following: <br><li>Execute test suite and capture results (external test or self-test) <br><li> Capture (log) the activity that resulted in a fault or that resulted in unexpected (perhaps emergent) behavior that was not necessarily a fault <br><li> Control and observe relevant system state for testing <br>Make sure the allocation of functionality provides high cohesion, low coupling, strong separation of concerns, and low structural complexity.
Coordination Model | Ensure the system’s coordination and communication mechanisms: <br><li> Support the execution of a test suite and capture the results within a system or between systems <br><li> Support capturing activity that resulted in a fault within a system or between systems <br><li> Support injection and monitoring of state into the communication channels for use in testing, within a system or between systems <br><li> Do not introduce needless nondeterminism
Data Model | Determine the major data abstractions that must be tested to ensure the correct operation of the system. <br><li> Ensure that it is possible to capture the values of instances of these data abstractions <br><li> Ensure that the values of instances of these data abstractions can be set when state is injected into the system, so that system state leading to a fault may be re-created <br><li> Ensure that the creation, initialization, persistence, manipulation, translation, and destruction of instances of these data abstractions can be exercised and captured
Mapping among Architectural Elements | Determine how to test the possible mappings of architectural elements (especially mappings of processes to processors, threads to processes, and modules to components) so that the desired test response is achieved and potential race conditions identified. <br>In addition, determine whether it is possible to test for illegal mappings of architectural elements.
Resource Management | Ensure there are sufficient resources available to execute a test suite and capture the results. Ensure that your test environment is representative of (or better yet, identical to) the environment in which the system will run. Ensure that the system provides the means to do the following: <br><li> Test resource limits <br><li> Capture detailed resource usage for analysis in the event of a failure <br><li> Inject new resource limits into the system for the purposes of testing <br><li> Provide virtualized resources for testing
Binding Time | Ensure that components that are bound later than compile time can be tested in the late-bound context. <br>Ensure that late bindings can be captured in the event of a failure, so that you can re-create the system’s state leading to the failure. <br>Ensure that the full range of binding possibilities can be tested.
Choice of Technology | Determine what technologies are available to help achieve the testability scenarios that apply to your architecture. Are technologies available to help with regression testing, fault injection, recording and playback, and so on? <br>Determine how testable the technologies are that you have chosen (or are considering choosing in the future) and ensure that your chosen technologies support the level of testing appropriate for your system. For example, if your chosen technologies do not make it possible to inject state, it may be difficult to re-create fault scenarios.

> **Now That Your Architecture Is Set to Help You Test . . .**
> 
> _By Nick Rozanski, coauthor (with Eoin Woods) of_ Software Systems Architecture: Working With Stakeholders Using Viewpoints and Perspectives
> 
> In addition to architecting your system to make it amenable to testing, you will need to overcome two more specific and daunting challenges when testing very large or complex systems, namely test data and test automation.
>
> _Test Data_
>
> Your first challenge is how to create large, consistent and useful test data sets. This is a significant problem in my experience, particularly for integration testing (that is, testing a number of components to confirm that they work together correctly) and performance testing (confirming that the system meets it requirements for throughput, latency, and response time). For unit tests, and usually for user acceptance tests, the test data is typically created by hand.
>
> For example, you might need 50 products, 100 customers, and 500 orders in your test database, so that you can test the functional steps involved in creating, amending, or deleting orders. This data has to be sufficiently varied to make testing worthwhile, it has to conform to all the referential integrity rules and other constraints of your data model, and you need to be able to calculate and specify the expected results of the tests.
>
> I’ve seen—and been involved in—two ways of doing this: you either write a system to generate your test data, or you capture a representative data set from the production environment and anonymize it as necessary. (Anonymizing test data involves removing any sensitive information, such as personal data about people or organizations, financial details, and so on.)
>
> Creating your own test data is the ideal, because you know what data you are using and can ensure that it covers all of your edge cases, but it is a lot of effort. Capturing data from the live environment is easier, assum-ing that there is a system there already, but you don’t know what data and hence what coverage you’re going to get, and you may have to take extra care to conform to privacy and data protection legislation.
>
> This can have an impact on the system’s architecture in a number of ways, and should be given due consideration early on by the architect. For example, the system may need to be able to capture live transactions, or take “snapshots” of live data, which can be used to generate test data. In ad-dition, the test-data-generation system may need an architecture of its own.
>
> _Test Automation_
> 
> Your second challenge is around test automation. In practice it is not pos-sible to test large systems by hand because of the number of tests, their complexity, and the amount of checking of results that’s required. In the ideal world, you create a test automation framework to do this automati-cally, which you feed with test data, and set running every night, or even run every time you check in something (the continuous integration model).
>
> This is an area that is given too little attention on many large software development projects. It is often not budgeted for in the project plan, with an unwritten assumption that the effort needed to build it can be somehow “absorbed” into the development costs. A test automation framework can be a significantly complex thing in its own right (which raises the question of how you test it!). It should be scoped and planned like any other project deliverable.
>
> Due consideration should be given to how the framework will invoke functions on the system under test, particularly for testing user interfaces, which is almost without exception a nightmare. (The execution of a UI test is highly dependent on the layout of the windows, the ordering of fields, and so on, which usually changes a lot in heavily user-focused systems. It is sometimes possible to execute window controls programmatically, but in the worst case you may have to record and replay keystrokes or mouse movements.)
>
> There are lots of tools to help with this nowadays, such as Quick Test Pro, TestComplete, or Selenium for testing, and CruiseControl, Hudson, and TeamCity for continuous integration. A comprehensive list on the web can be found here: en.wikipedia.org/wiki/Test_automation.
