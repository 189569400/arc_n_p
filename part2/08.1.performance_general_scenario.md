8.1 Performance General Scenario
===

A performance scenario begins with an event arriving at the system. Responding correctly to the event requires resources (including time) to be consumed. While this is happening, the system may be simultaneously servicing other events.

> **Concurrency**
> Concurrency is one of the more important concepts that an architect must understand and one of the least-taught in computer science courses. Concurrency refers to operations occurring in parallel. For example, sup-pose there is a thread that executes the statements
> ```
> x := 1;
> x++;
> ```
> and another thread that executes the same statements. What is the value of x after both threads have executed those statements? It could be either 2 or 3. I leave it to you to figure out how the value 3 could occur—or should I say I interleave it to you?
> 
> Concurrency occurs any time your system creates a new thread, be-cause threads, by definition, are independent sequences of control. Multi-tasking on your system is supported by independent threads. Multiple users are simultaneously supported on your system through the use of threads.
> 
> Concurrency also occurs any time your system is executing on more than one processor, whether the processors are packaged separately or as multi-core processors. In addition, you must consider concurrency when parallel algorithms, parallelizing infrastructures such as map-reduce, or NoSQL databases are used by your system, or you utilize one of a variety of concurrent scheduling algorithms. In other words, concurrency is a tool available to you in many ways.
>
> Concurrency, when you have multiple CPUs or wait states that can exploit it, is a good thing. Allowing operations to occur in parallel improves performance, because delays introduced in one thread allow the processor to progress on another thread. But because of the interleaving phenome-non just described (referred to as a race condition), concurrency must also be carefully managed by the architect.
> 
> As the example shows, race conditions can occur when there are two threads of control and there is shared state. The management of con-currency frequently comes down to managing how state is shared. One technique for preventing race conditions is to use locks to enforce sequen-tial access to state. Another technique is to partition the state based on the thread executing a portion of code. That is, if there are two instances of x in our example, x is not shared by the two threads and there will not be a race condition.
> 
> Race conditions are one of the hardest types of bugs to discover; the occurrence of the bug is sporadic and depends on (possibly minute) differ-ences in timing. I once had a race condition in an operating system that I could not track down. I put a test in the code so that the next time the race condition occurred, a debugging process was triggered. It took over a year for the bug to recur so that the cause could be determined.
> 
> Do not let the difficulties associated with concurrency dissuade you from utilizing this very important technique. Just use it with the knowledge that you must carefully identify critical sections in your code and ensure that race conditions will not occur in those sections.
> 
> —LB

Events can arrive in predictable patterns or mathematical distributions, or be unpredictable. An arrival pattern for events is characterized as *periodic*, *stochastic*, or *sporadic*:
* Periodic events arrive predictably at regular time intervals. For instance, an event may arrive every 10 milliseconds. Periodic event arrival is most often seen in real-time systems.
* Stochastic arrival means that events arrive according to some probabilistic distribution.
* Sporadic events arrive according to a pattern that is neither periodic nor stochastic. Even these can be characterized, however, in certain circumstances. For example, we might know that at most 600 events will occur in a minute, or that there will be at least 200 milliseconds between the arrival of any two events. (This might describe a system in which events correspond to keyboard strokes from a human user.) These are helpful characterizations, even though we don’t know when any single event will arrive.

The response of the system to a stimulus can be measured by the following:

* *Latency*. The time between the arrival of the stimulus and the system’s response to it. 
* *Deadlines in processing*. In the engine controller, for example, the fuel should ignite when the cylinder is in a particular position, thus introducing a processing deadline.
* The *throughput of* the system, usually given as the number of transactions the system can process in a unit of time.
* The *jitter* of the response—the allowable variation in latency.
* The *number of events not processed* because the system was too busy to respond. 

From these considerations we can now describe the individual portions of a general scenario for performance:

* *Source of stimulus*. The stimuli arrive either from external (possibly multiple) or internal sources.
* *Stimulus*. The stimuli are the event arrivals. The arrival pattern can be peri-odic, stochastic, or sporadic, characterized by numeric parameters.
* *Artifact*. The artifact is the system or one or more of its components.
* *Environment*. The system can be in various operational modes, such as nor-mal, emergency, peak load, or overload.
* *Response*. The system must process the arriving events. This may cause a change in the system environment (e.g., from normal to overload mode).
* *Response measure*. The response measures are the time it takes to process the arriving events (latency or a deadline), the variation in this time (jitter), the number of events that can be processed within a particular time interval (throughput), or a characterization of the events that cannot be processed (miss rate).

The general scenario for performance is summarized in Table 8.1.

Figure 8.1 gives an example concrete performance scenario: Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds.

TABLE 8.1 Performance General Scenario


Portion of Scenario | Possible Values
---|--
Source | Internal or external to the system
Stimulus | Arrival of a periodic, sporadic, or stochastic event
Artifact | System or one or more components in the system
Environment | Operational mode: normal, emergency, peak load, overload
Response | Process events, change level of service
Response Measure | Latency, deadline, throughput, jitter, miss rate