12.3 Software Quality Attributes and System Quality Attributes 软件质量属性和系统质量属性
===

Physical systems, such as aircraft or automobiles or kitchen appliances, that rely on software embedded within are designed to meet a whole other litany of qual-ity attributes: weight, size, electric consumption, power output, pollution output, weather resistance, battery life, and on and on. For many of these systems, safety tops the list (see the sidebar).

Sometimes the software architecture can have a surprising effect on the sys-tem’s quality attributes. For example, software that makes inefficient use of com-puting resources might require additional memory, a faster processor, a bigger battery, or even an additional processor. Additional processors can add to a sys-tem’s power consumption, weight, required cabinet space, and of course expense.

Green computing is an issue of growing concern. Recently there was a con-troversy about how much greenhouse gas was pumped into the atmosphere by Google’s massive processor farms. Given the daily output and the number of daily requests, it is possible to estimate how much greenhouse gas you cause to be emitted each time you ask Google to perform a search. (Current estimates range from 0.2 grams to 7 grams of CO 2 .) Green computing is all the rage. Eve Troeh, on the American Public Media show “Marketplace” (July 5, 2011), reports:

  Two percent of all U.S. electricity now goes to data centers, according to the Environmental Protection Agency. Electricity has become the biggest cost for processing data—more than the equipment to do it, more than the buildings to house that equipment. . . . Google’s making data servers that can float offshore, cooled by ocean breezes. HP has plans to put data servers near farms, and power them with methane gas from cow pies.

The lesson here is that if you are the architect for software that resides in a larger system, you will need to understand the quality attributes that are import-ant for the containing system to achieve, and work with the system architects and engineers to see how your software architecture can contribute to achieving them.

> **The Vanishing Line between Software and System Qualities**
This is a book about software architecture, and so we treat quality attri-butes from a software architect’s perspective. But you may have already noticed that the quality attributes that the software architect can bring to the party are limited by the architecture of the system in which the soft-ware runs.
>
> For example:
> * The performance of a piece of software is fundamentally constrained by the performance of the computer that runs it. No matter how well you design the software, you just can’t run the latest whole-earth weather forecasting models on Grampa’s Commodore 64 and hope to know if it’s going to rain tomorrow.
> * Physical security is probably more important and more effective than software security at preventing fraud and theft. If you don’t believe this, write your laptop’s password on a slip of paper, tape it to your laptop, and leave it in an unlocked car with the windows down. (Actually, don’t really do that. Consider this a thought experiment.)
> * If we’re being perfectly honest here, how usable is a device for web browsing that has a screen smaller than a credit card and keys the size of a raisin?
> 
> For me, nowhere is the barrier between software and system more nebulous than in the area of safety. The thought that software—strings of 0’s and 1’s—can kill or maim or destroy is still an unnatural notion. Of course, it’s not the 0’s and 1’s that wreak havoc. At least, not directly. It’s what they’re connected to. Software, and the system in which it runs, has to be connected to the outside world in some way before it can do damage. That’s the good news. The bad news is that the good news isn’t all that good. Software is connected to the outside world, always. If your program has no effect whatsoever that is observable outside of itself, it probably serves no purpose.
>
> There are notorious examples of software-related failures. The Siberian hydroelectric plant catastrophe mentioned in the text, the Therac-25 fatal radiation overdose, the Ariane 5 explosion, and a hundred lesser known accidents all caused harm because the software was part of a system that included a turbine, an X-ray emitter, or a rocket’s steering controls, in the examples just cited. In these cases, flawed software commanded some hardware in the system to take a disastrous action, and the hardware sim-ply obeyed. Actuators are devices that connect hardware to software; they are the bridge between the world of 0’s and 1’s and the world of motion and control. Send a digital value to an actuator (or write a bit string in the hard-ware register corresponding to the actuator) and that value is translated to some mechanical action, for better or worse.
>
> But connection to an actuator is not required for software-related disas-ters. Sometimes all the computer has to do is send erroneous information to its human operators. In September 1983, a Soviet satellite sent data to its ground system computer, which interpreted that data as a missile launched from the United States aimed at Moscow. Seconds later, the computer reported a second missile in flight. Soon, a third, then a fourth, and then a fifth appeared. Soviet Strategic Rocket Forces lieutenant colonel Stanislav evgrafovich Petrov made the astonishing decision to ignore the warning system, believing it to be in error. He thought it extremely unlikely that the U.S. would have fired just a few missiles, thereby inviting total retaliatory destruction. He decided to wait it out, to see if the missiles were real—that is, to see if his country’s capital city was going to be incinerated. As we know, it wasn’t. The Soviet system had mistaken a rare sunlight con-dition for missiles in flight. Similar mistakes have occurred on the U.S. side.
>
> Of course, the humans don’t always get it right. On the dark and stormy night of June 1, 2009, Air France flight 447 from Rio de Janeiro to Paris plummeted into the Atlantic Ocean, killing all on board. The Airbus A-330’s flight recorders were not recovered until May 2011, and as this book goes to publication it appears that the pilots never knew that the aircraft had en-tered a high-altitude stall. The sensors that measure airspeed had become clogged with ice and therefore unreliable. The software was required to dis-engage the autopilot in this situation, which it did. The human pilots thought the aircraft was going too fast (and in danger of structural failure) when in fact it was going too slow (and falling). During the entire three-minute-plus plunge from 38,000 feet, the pilots kept trying to pull the nose up and throt-tles back to lower the speed. It’s a good bet that adding to the confusion was the way the A-330’s stall warning system worked. When the system detects a stall, it emits a loud audible alarm. The computers deactivate the stall warning when they “think” that the angle of attack measurements are invalid. This can occur when the airspeed readings are very low. That is ex-actly what happened with Air France 447: Its forward speed dropped below 60 knots, and the angle of attack was extremely high. As a consequence of a rule in the flight control software, the stall warning stopped and started several times. Worse, it came on whenever the pilot let the nose fall a bit (increasing the airspeed and taking the readings into the “valid” range, but still in stall) and then stopped when he pulled back. That is, doing the right thing resulted in the wrong feedback and vice versa.
>
> Was this an unsafe system, or a safe system unsafely operated?
Ultimately the courts will decide.
> 
> Software that can physically harm us is a fact of our modern life. Sometimes the link between software and physical harm is direct, as in the Ariane example, and sometimes it’s much more tenuous, as in the Air France 447 example. But as software professionals, we cannot take refuge in the fact that our software can’t actually inflict harm any more than the person who shouts “Fire!” in a crowded theater can claim it was the stam-pede, not the shout, that caused injury.
>
> —PCC
