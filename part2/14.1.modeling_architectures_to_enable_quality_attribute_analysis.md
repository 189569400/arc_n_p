14.1 Modeling Architectures to Enable Quality Attribute Analysis
===

Some quality attributes, most notably performance and availability, have well-un-derstood, time-tested analytic models that can be used to assist in an analysis. By analytic model, we mean one that supports quantitative analysis. Let us first consider performance.

## Analyzing Performance

In Chapter 12 we discussed the fact that models have parameters, which are val-ues you can set to predict values about the entity being modeled (and in Chap-ter 12 we showed how to use the parameters to help us derive tactics for the quality attribute associated with the model). As an example we showed a queuing model for performance as Figure 12.2, repeated here as Figure 14.1. The parame-ters of this model are the following:
* The arrival rate of events
* The chosen queuing discipline
* The chosen scheduling algorithm
* The service time for events
* The network topology
* The network bandwidth
* The routing algorithm chosen

In this section, we discuss how such a model can be used to understand the latency characteristics of an architectural design.

To apply this model in an analytical fashion, we also need to have previ-ously made some architecture design decisions. We will use model-view-control-ler as our example here. MVC, as presented in Section 13.2, says nothing about its deployment. That is, there is no specification of how the model, the view, and the controller are assigned to processes and processors; that’s not part of the pat-tern’s concern. These and other design decisions have to be made to transform a pattern into an architecture. Until that happens, one cannot say anything with authority about how an MVC-based implementation will perform. For this exam-ple we will assume that there is one instance each of the model, the view, and the controller, and that each instance is allocated to a separate processor. Figure 14.2 shows MVC following this allocation scheme.

![](fig.14.1)
FIGURE 14.1 A queuing model of performance

![](fig.14.2)
FIGURE 14.2 An allocation view, in UML, of a model-view-controller architecture

Given that quality attribute models such as the performance model shown in Figure 14.1 already exist, the problem becomes how to map these allocation and coordination decisions onto Figure 14.1. Doing this yields Figure 14.3. There are requests coming from users outside the system—labeled as 1 in Fig-ure 14.3—arriving at the view. The view processes the requests and sends some transformation of the requests on to the controller—labeled as 2. Some actions of the controller are returned to the view—labeled as 3. The controller sends other actions on to the model—labeled 4. The model performs its activities and sends information back to the view—labeled 5.

To analyze the model in Figure 14.3, a number of items need to be known or estimated:
* The frequency of arrivals from outside the system
* The queuing discipline used at the view queue
* The time to process a message within the view
* The number and size of messages that the view sends to the controller
* The bandwidth of the network that connects the view and the controller
* The queuing discipline used by the controller
* The time to process a message within the controller
* The number and size of messages that the controller sends back to the view
* The bandwidth of the network used for messages from the controller to the view
* The number and size of messages that the controller sends to the model
* The queuing discipline used by the model
* The time to process a message within the model
* The number and size of messages the model sends to the view
* The bandwidth of the network connecting the model and the view

![](fig.14.3)
FIGURE 14.3 A queuing model of performance for MVC

Given all of these assumptions, the latency for the system can be estimated. Sometimes well-known formulas from queuing theory apply. For situations where there are no closed-form solutions, estimates can often be obtained through sim-ulation. Simulations can be used to make more-realistic assumptions such as the distribution of the event arrivals. The estimates are only as good as the assump-tions, but they can serve to provide rough values that can be used either in design or in evaluation; as better information is obtained, the estimates will improve.

A reasonably large number of parameters must be known or estimated to construct the queuing model shown in Figure 14.3. The model must then be solved or simulated to derive the expected latency. This is the cost side of the cost/benefit of performing a queuing analysis. The benefit side is that as a result of the analysis, there is an estimate for latency, and “what if” questions can be easily answered. The question for you to decide is whether having an estimate of the latency and the ability to answer “what if” questions is worth the cost of per-forming the analysis. One way to answer this question is to consider the impor-tance of having an estimate for the latency prior to constructing either the system or a prototype that simulates an architecture under an assumed load. If having a small latency is a crucial requirement upon which the success of the system re-lies, then producing an estimate is appropriate.

Performance is a well-studied quality attribute with roots that extend beyond the computer industry. For example, the queuing model given in Figure 14.1 dates from the 1930s. Queuing theory has been applied to factory floors, to bank-ing queues, and to many other domains. Models for real-time performance, such as rate monotonic analysis, also exist and have sophisticated analysis techniques. 

## Analyzing Availability

Another quality attribute with a well-understood analytic framework is availability.

Modeling an architecture for availability—or to put it more carefully, mod-eling an architecture to determine the availability of a system based on that archi-tecture—is a matter of determining the failure rate and the recovery time. As you may recall from Chapter 5, availability can be expressed as

$
\frac{MTBF}{( MTBF + MTTR )}
$

This models what is known as steady-state availability, and it is used to indicate the uptime of a system (or component of a system) over a sufficiently long duration. In the equation, MTBF is the mean time between failure, which is derived based on the expected value of the implementation’s failure probability density function (PDF), and MTTR refers to the mean time to repair.

Just as for performance, to model an architecture for availability, we need an architecture to analyze. So, suppose we want to increase the availability of a system that uses the broker pattern, by applying redundancy tactics. Figure 14.4

illustrates three well-known redundancy tactics from Chapter 5: active redun-dancy, passive redundancy, and cold spare. Our goal is to analyze each redun-dancy option for its availability, to help us choose one.

As you recall, each of these tactics introduces a backup copy of a compo-nent that will take over in case the primary component suffers a failure. In our case, a broker replica is employed as the redundant spare. The difference among them is how up to date with current events each backup keeps itself:
* In the case of active redundancy, the active and redundant brokers both receive identical copies of the messages received from the client and server proxies. The internal broker state is synchronously maintained between the active and redundant spare in order to facilitate rapid failover upon detec-tion of a fault in the active broker.
* For the passive redundancy implementation, only the active broker receives and processes messages from the client and server proxies. When using this tactic, checkpoints of internal broker state are periodically transmitted from the active broker process to the redundant spare, using the checkpoint-based rollback tactic.
* Finally, when using the cold spare tactic, only the active broker receives and processes messages from the client and server proxies, because the redundant spare is in a dormant or even powered-off state. Recovery strate-gies using this tactic involve powering up, booting, and loading the broker implementation on the spare. In this scenario, the internal broker state is rebuilt organically, rather than via synchronous operation or checkpointing, as described for the other two redundancy tactics.

Suppose further that we will detect failure with the heartbeat tactic, where each broker (active and spare) periodically transmits a heartbeat message to a separate process responsible for fault detection, correlation, reporting, and recov-ery. This fault manager process is responsible for coordinating the transition of the active broker role from the failed broker process to the redundant spare.

You can now use the steady state model of availability to assign values for ${MTBF}$ and ${MTTR}$ for each of the three redundancy tactics we are considering. Doing so will be an exercise left to the reader (as you’ll see when you reach the discussion questions for this chapter). Because the three tactics differ primarily in how long it takes to bring the backup copy up to speed, ${MTTR}$ will be where the difference among the tactics shows up.

More sophisticated models of availability exist, based on probability. In these models, we can express a probability of failure during a period of time. Given a particular ${MTBF}$ and a time duration T, the probability of failure ${R} is given by

$
R(T)= e\left ( \frac{-T}{MTBF} \right )
$

![](fig.14.4)
FIGURE 14.4 Redundancy tactics, as applied to a broker pattern 

You will recall from Statistics 101 that:
* When two events A and B are independent, the probability that A or B will occur is the sum of the probability of each event: $P(A or B) = P(A) + P(B)$.
* When two events A and B are independent, the probability of both occur-ring is $P(A and B) = P(A) \cdot P(B)$.
* When two events A and B are dependent, the probability of both occurring is $P(A and B) = P(A) \cdot P(B|A)$, where the last term means “the probability of B occurring, given that A occurs.”

We can apply simple probability arithmetic to an architecture pattern for availability to determine the probability of failure of the pattern given the proba-bility of failure of the individual components (and an understanding of their de-pendency relations). For example, in an architecture pattern employing the pas-sive redundancy tactic, let’s assume that the failure of a component (which at any moment might be acting as either the primary or backup copy) is independent of a failure of its counterpart, and that the probability of failure of either is the same. Then the probability that both will fail is $F = (1 – a) **2$, where a is the availability of an individual component (assuming that failures are independent).

Still other models take into account different levels of failure severity and degraded operating states of the system. Although the derivation of these for-mulas is outside the scope of this chapter, you end up with formulas that look like the following for the three redundancy tactics we’ve been discussing, where the values C2 through C5 are references to the MTBF column of Table 14.1, D2 through D4 refer to the Active column, E2 through E3 refer to the Passive col-umn, and F2 through F3 refer to the Spare column.
* Active redundancy:
  * Availability(MTTR): 1 –((SUM(C2:C5) + D3) × D2)/((C2 × (C2 + C4 + D3) + ((C2 + C4 + D2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + D3))))
  * P(Degraded) = ((C3 + C5) × D2)/((C2 × (C2 + C4 + D3) + ((C2 + C4 + D2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + D3))))
* Passive redundancy:
  * Availability(MTTR_passive) = 1 – ((SUM(C2:C5) + E3) × E2)/((C2 × (C2 + C4 + E3) + ((C2 + C4 + E2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + E3))))
  * P(Degraded) = ((C3 + C5) × E2)/((C2 × (C2 + C4 + E3) + ((C2 + C4 + E2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + E3))))
* Spare:
  * Availability(MTTR) = 1 – ((SUM(C2:C5) + F3) × F2)/((C2 × (C2 + C4 + F3) + ((C2 + C4 + F2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + F3))))
  * P(Degraded) = ((C3 + C5) × F2)/((C2 × (C2 + C4 + F3) + ((C2 + C4 + F2) × (C3 + C5)) + ((C2 + C4) × (C2 + C4 + F3))))

Plugging in these values for the parameters to the equations listed above results in a table like Table 14.1, which can be easily calculated using any spread-sheet tool. Such a calculation can help in the selection of tactics.

TABLE 14.1 Calculated Availability for an Availability-Enhanced Broker Implementation

-|-|-|-|MTTR (Seconds)|-
---|---|---|---|---|---
Function | Failure Severity | MTBF (Hours) | Active Redundancy (Hot Spare) | Passive Redundancy (Warm Spare) | Spare (Cold Spare)
Hardware | 1 | 250,000 | 1 | 5 | 900
 -       | 2 |  50,000 | 30 | 30 | 30
Software | 1 |  50,000 | 1 | 5 | 900
 -       | 2 |  10,000 | 30 | 30 | 30
Availability |  | | 0.9999998 | 0.999990 | 0.9994

## The Analytic Model Space

As we discussed in the preceding sections, there are a growing number of analytic models for some aspects of various quality attributes. One of the quests of software engineering is to have a sufficient number of analytic models for a sufficiently large number of quality attributes to enable prediction of the behavior of a designed sys-tem based on these analytic models. Table 14.2 shows our current status with respect to this quest for the seven quality attributes discussed in Chapters 5–11.

TABLE 14.2 A Summary of the Analytic Model Space

Quality Attribute | Intellectual Basis | Maturity/Gaps
---|---|---
Availability | Markov models; statistical models | Moderate maturity; mature in the hardware reliability domain, less mature in the software domain. Requires models that speak to state recovery and for which failure percentages can be attributed to software.
Interoperability | Conceptual framework | Low maturity; models require substantial human interpretation and input.
Modifiability | Coupling and cohesion metrics; cost models | Substantial research in academia; still requires more empirical support in real-world environments.
Performance | Queuing theory; real-time scheduling theory | High maturity; requires considerable education and training to use properly.
Security | No architectural models | 
Testability | Component interaction metrics | Low maturity; little empirical validation.
Usability | No architectural models | 

As the table shows, the field still has a great deal of work to do to achieve the quest for well-validated analytic models to predict behavior, but there is a great deal of activity in this area (see the “For Further Reading” section for ad-ditional papers). The remainder of this chapter deals with techniques that can be used in addition to analytic models.
