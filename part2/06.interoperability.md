6 Interoperability 可操作性
===

_With Liming Zhu_

_The early bird (A) arrives and catches worm (B), pulling
string (C) and shooting off pistol (D). Bullet (E) bursts
balloon (F), dropping brick (G) on bulb (H) of atomizer
(I) and shooting perfume (J) on sponge (K). As sponge
gains in weight, it lowers itself and pulls string (L),
raising end of board (M). Cannon ball (N) drops on nose
of sleeping gentleman. String tied to cannon ball releases
cork (O) of vacuum bottle (P) and ice water falls on
sleeper’s face to assist the cannon ball in its good work._  
—Rube Goldberg, instructions for “a simple alarm clock”

Interoperability is about the degree to which two or more systems can usefully exchange meaningful information via interfaces in a particular context. The definition includes not only having the ability to exchange data (syntactic interoperability) but also having the ability to correctly interpret the data being exchanged (semantic interoperability). A system cannot be interoperable in isolation. Any discussion of a system’s interoperability needs to identify with whom, with what, and under what circumstances—hence, the need to include the context.

Interoperability is affected by the systems expected to interoperate. If we already know the interfaces of external systems with which our system will interoperate, then we can design that knowledge into the system. Or we can design our system to interoperate in a more generic fashion, so that the identity and the services that another system provides can be bound later in the life cycle, at build time or runtime.

Like all quality attributes, interoperability is not a yes-or-no proposition but has shades of meaning. There are several characterizing frameworks for interoperability, all of which seem to define five levels of interoperability “maturity” (see the “For Further Reading” section at the end of this chapter for a pointer). The lowest level signifies systems that do not share data at all, or do not do so with any success. The highest level signifies systems that work together seamlessly, never make any mistakes interpreting each other’s communications, and share the same underlying semantic model of the world in which they work.

> **“Exchanging Information via Interfaces”**
>
>Interoperability, as we said, is about two or more systems exchanging information via interfaces.
>
>At this point, we need to clarify two critical concepts central to this discussion and emphasize that we are taking a broad view of each.
>
>The first is what it means to “exchange information.” This can mean something as simple as program A calling program B with some parameters. However, two systems (or parts of a system) can exchange information even if they never communicate directly with each other. Did you ever have a conversation like the following in junior high school? “Charlene said that Kim told her that Trevor heard that Heather wants to come to your party.” Of course, junior high school protocol would preclude the possibility of responding directly to Heather. Instead, your response (if you like Heather) might be, “Cool,” which would make its way back through Charlene, Kim, and Trevor. You and Heather exchanged information, but never talked to each other. (We hope you got to talk to each other at the party.)
>
>Entities can exchange information in even less direct ways. If I have an idea of a program’s behavior, and I design my program to work assuming that behavior, the two programs have also exchanged information—just not at runtime.
>
>One of the more infamous software disasters in history occurred when an antimissile system failed to intercept an incoming ballistic rocket in Operation Desert Storm in 1991, resulting in 28 fatalities. One of the missile’s software components “expected” to be shut down and restarted periodically, so it could recalibrate its orientation framework from a known initial point. The software had been running for some 100 hours when the missile was launched, and calculation errors had accumulated to the point where the software component’s idea of its orientation had wandered hopelessly away from truth.
>
>Systems (or components within systems) often have or embody expectations about the behaviors of its “information exchange” partners. The assumption of everything interacting with the errant component in the preceding example was that its accuracy did not degrade over time. The result was a system of parts that did not work together correctly to solve the problem they were supposed to.
>
>The second concept we need to stress is what we mean by “interface.” Once again, we mean something beyond the simple case—a syntactic description of a component’s programs and the type and number of their parameters, most commonly realized as an API. That’s necessary for interoperability—heck, it’s necessary if you want your software to compile successfully—but it’s not sufficient. To illustrate this concept, we’ll use another “conversation” analogy. Has your partner or spouse ever come home, slammed the door, and when you ask what’s wrong, replied “Nothing!”? If so, then you should be able to appreciate the keen difference between syntax and semantics and the role of expectations in understanding how an entity behaves. Because we want interoperable systems and components, and not simply ones that compile together nicely, we require a higher bar for interfaces than just a statement of syntax. By “interface,” we mean the set of assumptions that you can safely make about an entity. For example, it’s a safe assumption that whatever’s wrong with your spouse/partner, it’s not “Nothing,” and you know that because that “interface” extends way beyond just the words they say. And it’s also a safe assumption that nothing about our missile component’s accuracy degradation over time was in its API, and yet that was a critical part of its interface.
>
> —PCC

Here are some of the reasons you might want systems to interoperate:

* Your system provides a service to be used by a collection of unknown systems. These systems need to interoperate with your system even though you may know nothing about them. An example is a service such as Google Maps.
* You are constructing capabilities from existing systems. For example, one of the existing systems is responsible for sensing its environment, another one is responsible for processing the raw data, a third is responsible for interpreting the data, and a final one is responsible for producing and distributing a representation of what was sensed. An example is a traffic sensing system where the input comes from individual vehicles, the raw data is processed into common units of measurement, is interpreted and fused, and traffic congestion information is broadcast.

These examples highlight two important aspects of interoperability:

1. Discovery. The consumer of a service must discover (possibly at runtime, possibly prior to runtime) the location, identity, and the interface of the service.
2. Handling of the response. There are three distinct possibilities:
   * The service reports back to the requester with the response.
   * The service sends its response on to another system.
   * The service broadcasts its response to any interested parties.

These elements, discovery and disposition of response, along with management of interfaces, govern our discussion of scenarios and tactics for interoperability.

> Systems of Systems
> If you have a group of systems that are interoperating to achieve a joint purpose, you have what is called a system of systems (SoS). An SoS is an arrangement of systems that results when independent and useful systems are integrated into a larger system that delivers unique capabilities. Table 6.1 shows a categorization of SoSs.
> 
> ## Table 6.1 Taxonomy of Systems of Systems*
> _ | _ 
> --|--
> Directed | SoS objectives, centralized management, funding, and authority for the overall SoS are in place. Systems are subordinated to the SoS.
> Acknowledged | SoS objectives, centralized management, funding, and authority in place. However, systems retain their own management, funding, and authority in parallel with the SoS.
> Collaborative | There are no overall objectives, centralized management, authority, responsibility, or funding at the SoS level. Systems voluntarily work together to address shared or common interests.
> Virtual | Like collaborative, but systems don’t know about each other.
> 
> _* The taxonomy shown is an extension of work done by Mark Maier in 1998._
> 
> In directed and acknowledged SoSs, there is a deliberate attempt to create an SoS. The key difference is that in the former, there is SoS-level management that exercises control over the constituent systems, while in the latter, the constituent systems retain a high degree of autonomy in their own evolution. Collaborative and virtual systems of systems are more ad hoc, absent an overarching authority or source of funding and, in the case of a virtual SoS, even absent the knowledge about the scope and membership of the SoS.
> 
> The collaborative case is quite common. Consider the Google Maps example from the introduction. Google is the manager and funding authority for the map service. Each use of the maps in an application (an SoS) has its own management and funding authority, and there is no overall management of all of the applications that use Google Maps. The various organizations involved in the applications collaborate (either explicitly or implicitly) to enable the applications to work correctly.
> 
> A virtual SoS involves large systems and is much more ad hoc. For example, there are over 3,000 electric companies in the U.S. electric grid, each state has a public utility commission that oversees the utility companies operating in its state, and the federal Department of Energy provides some level of policy guidance. Many of the systems within the electric grid must interoperate, but there is no management authority for the overall system.