14.4 Experiments, Simulations, and Prototypes
===

In many environments it is virtually impossible to do a purely top-down architec-tural design; there are too many considerations to weigh at once and it is too hard to predict all of the relevant technological barriers. Requirements may change in dramatic ways, or a key assumption may not be met: We have seen cases where a vendor-provided API did not work as specified, or where an API exposing a criti-cal function was simply missing.

Finding the sweet spot within the enormous architectural design space of complex systems is not feasible by reflection and mathematical analysis alone; the models either aren’t precise enough to deal with all of the relevant details or are so complicated that they are impractical to analyze with tractable mathemat-ical techniques.

The purpose of _experiments_, _simulations_, and _prototypes_ is to provide al-ternative ways of analyzing the architecture. These techniques are invaluable in resolving tradeoffs, by helping to turn unknown architectural parameters into constants or ranges. For example, consider just a few of the questions that might occur when creating a web-conferencing system—a distributed client-server in-frastructure with real-time constraints:
* Would moving to a distributed database from local flat files negatively im-pact feedback time (latency) for users?
* How many participants could be hosted by a single conferencing server?
* What is the correct ratio between database servers and conferencing servers?

These sorts of questions are difficult to answer analytically. The answers to these questions rely on the behavior and interaction of third-party components such as commercial databases, and on performance characteristics of software for which no standard analytical models exist. The approach used for the web-con-ferencing architecture was to build an extensive testing infrastructure that sup-ported simulations, experiments, and prototypes, and use it to compare the per-formance of each incremental modification to the code base. This allowed the architect to determine the effect of each form of improvement before committing to including it in the final system. The infrastructure includes the following:
* A client simulator that makes it appear as though tens of thousands of cli-ents are simultaneously interacting with a conferencing server.
* Instrumentation to measure load on the conferencing server and database server with differing numbers of clients.

The lesson from this experience is that experimentation can often be a criti-cal precursor to making significant architectural decisions. Experimentation must be built into the development process: building experimental infrastructure can be time-consuming, possibly requiring the development of custom tools. Carry-ing out the experiments and analyzing their results can require significant time. These costs must be recognized in project schedules.
