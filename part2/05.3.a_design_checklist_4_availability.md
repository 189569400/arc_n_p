5.3 A Design Checklist for Availability
===

Table 5.4 is a checklist to support the design and analysis process for availability.

Table 5.4 Checklist to Support the Design and Analysis Process for Availability

Category | Checklist
--|--
Allocation of Responsibilities | Determine the system responsibilities that need to be highly available. Within those responsibilities, ensure that additional responsibilities have been allocated to detect an omission, crash, incorrect timing, or incorrect response. Additionally, ensure that there are responsibilities to do the following: <br> <li> Log the fault <br> <li> Notify appropriate entities (people or systems) <br> <li> Disable the source of events causing the fault <br> <li> Be temporarily unavailable <br> <li> Fix or mask the fault/failure <br> <li> Operate in a degraded mode
Coordination Model | Determine the system responsibilities that need to be highly available. With respect to those responsibilities, do the following: <br> <li> Ensure that coordination mechanisms can detect an omission, crash, incorrect timing, or incorrect response. Consider, for example, whether guaranteed delivery is necessary. Will the coordination work under conditions of degraded communication? <br> <li> Ensure that coordination mechanisms enable the logging of the fault, notification of appropriate entities, disabling of the source of the events causing the fault, fixing or masking the fault, or operating in a degraded mode. <br> <li> Ensure that the coordination model supports the replacement of the artifacts used (processors, communications channels, persistent storage, and processes). For example, does replacement of a server allow the system to continue to operate? <br> <li> Determine if the coordination will work under conditions of degraded communication, at startup/shutdown, in repair mode, or under overloaded operation. For example, how much lost information can the coordination model withstand and with what consequences?
Data Model | Determine which portions of the system need to be highly available. Within those portions, determine which data abstractions, along with their operations or their properties, could cause a fault of omission, a crash, incorrect timing behavior, or an incorrect response. <br> For those data abstractions, operations, and properties, ensure that they can be disabled, be temporarily unavailable, or be fixed or masked in the event of a fault. <br> For example, ensure that write requests are cached if a server is temporarily unavailable and performed when the server is returned to service.
Mapping among Architectural Elements | Determine which artifacts (processors, communication channels, persistent storage, or processes) may produce a fault: omission, crash, incorrect timing, or incorrect response. <br> Ensure that the mapping (or remapping) of architectural elements is flexible enough to permit the recovery from the fault. This may involve a consideration of the following:  <br> <li> Which processes on failed processors need to be reassigned at runtime <br> <li> Which processors, data stores, or communication channels can be activated or reassigned at runtime <br> <li> How data on failed processors or storage can be served by replacement units <br> <li> How quickly the system can be reinstalled based on the units of delivery provided <br> <li> How to (re)assign runtime elements to processors, communication channels, and data stores  <br> <li> When employing tactics that depend on redundancy of functionality, the mapping from modules to redundant components is important. For example, it is possible to write one module that contains code appropriate for both the active component and backup components in a protection group.
Resource Management | Determine what critical resources are necessary to continue operating in the presence of a fault: omission, crash, incorrect timing, or incorrect response. Ensure there are sufficient remaining resources in the event of a fault to log the fault; notify appropriate entities (people or systems); disable the source of events causing the fault; be temporarily unavailable; fix or mask the fault/failure; operate normally, in startup, shutdown, repair mode, degraded operation, and overloaded operation. <br> Determine the availability time for critical resources, what critical resources must be available during specified time intervals, time intervals during which the critical resources may be in a degraded mode, and repair time for critical resources. Ensure that the critical resources are available during these time intervals. <br> For example, ensure that input queues are large enough to buffer anticipated messages if a server fails so that the messages are not permanently lost.

Table 5.4 Checklist to Support the Design and Analysis Process for
Availability, continued

Category | Checklist
--|--
Binding Time | Determine how and when architectural elements are bound. If late binding is used to alternate between components that can themselves be sources of faults (e.g., processes, processors, communication channels), ensure the chosen availability strategy is sufficient to cover faults introduced by all sources. For example: <br> <li> If late binding is used to switch between artifacts such as processors that will receive or be the subject of faults, will the chosen fault detection and recovery mechanisms work for all possible bindings? <br> <li> If late binding is used to change the definition or tolerance of what constitutes a fault (e.g., how long a process can go without responding before a fault is assumed), is the recovery strategy chosen sufficient to handle all cases? For example, if a fault is flagged after 0.1 milliseconds, but the recovery mechanism takes 1.5 seconds to work, that might be an unacceptable mismatch. <br> <li> What are the availability characteristics of the late binding mechanism itself? Can it fail? 
Choice of Technology | Determine the available technologies that can (help) detect faults, recover from faults, or reintroduce failed components. <br> Determine what technologies are available that help the response to a fault (e.g., event loggers). <br> Determine the availability characteristics of chosen technologies themselves: What faults can they recover from? What faults might they introduce into the system?