## 5 Avaibility 可用性

<!--https://blog.csdn.net/susemm/article/details/122770649-->

_With James Scott_

_Ninety percent of life is just showing up._

_百分之九十的生命都只是呈现而已。_

—Woody Allen 伍迪·艾伦

Availability refers to a property of software that it is there and ready to carry out its task when you need it to be. This is a broad perspective and encompasses what is normally called reliability (although it may encompass additional considerations such as downtime due to periodic maintenance). In fact, availability builds upon the concept of reliability by adding the notion of recovery—that is, when the system breaks, it repairs itself. Repair may be accomplished by various means, which we’ll see in this chapter. More precisely, Avižienis and his colleagues have defined dependability:

可用性指的是：可以在需要时使用它并准备执行其任务的软件属性。 这是一个广阔的视角，涵盖了通常所说的可靠性（尽管它可能包含其他考虑因素，例如由于定期维护而导致的停机）。 实际上，可用性通过添加恢复的概念建立在可靠性的概念上，也就是说，当系统崩溃时，它会自我修复。 维修可以通过多种方式完成，我们将在本章中介绍。 更准确地说，Avižienis和他的同事定义了可靠性：

Dependability is the ability to avoid failures that are more frequent and more severe than is acceptable.

可靠性是避免发生比可接受的更为频繁和严重的故障的能力。

Our definition of availability as an aspect of dependability is this: “Availability refers to the ability of a system to mask or repair faults such that the cumulative service outage period does not exceed a required value over a specified time interval.” These definitions make the concept of failure subject to the judgment of an external agent, possibly a human. They also subsume concepts of reliability, confidentiality, integrity, and any other quality attribute that involves a concept of unacceptable failure.

我们将可用性定义为可靠性的一个方面是：“可用性是指系统掩盖或修复故障的能力，以使累积的服务中断时间段在指定的时间间隔内不超过要求的值。” 失败的概念取决于外部因素（可能是人类）的判断。 它们还包含可靠性、机密性、完整性和涉及不可接受的故障的任何其他质量属性的概念。

Availability is closely related to security. A denial-of-service attack is explicitly designed to make a system fail—that is, to make it unavailable. Availability is also closely related to performance, because it may be difficult to tell when a system has failed and when it is simply being outrageously slow to respond. Finally, availability is closely allied with safety, which is concerned with keeping the system from entering a hazardous state and recovering or limiting the damage when it does.

可用性与安全性密切相关。 拒绝服务攻击是专门设计用来使系统发生故障的，也就是使其不可用。 可用性也与性能密切相关，因为可能很难分辨出系统何时发生故障以及何时响应变得异常缓慢。 最后，可用性与安全性密切相关，安全性与防止系统进入危险状态以及在系统损坏时恢复或限制损坏有关。

Fundamentally, availability is about minimizing service outage time by mitigating faults. Failure implies visibility to a system or human observer in the environment. That is, a failure is the deviation of the system from its specification, where the deviation is externally visible. One of the most demanding tasks in building a high-availability, fault-tolerant system is to understand the nature of the failures that can arise during operation (see the sidebar “Planning for Failure”). Once those are understood, mitigation strategies can be designed into the software.

从根本上讲，可用性是通过减少故障来最大程度地减少服务中断时间。 故障意味着对环境中的系统或人类观察者可见。 也就是说，故障是系统与其规格的偏差，该偏差在外部是可见的。 构建高可用性，容错系统中最艰巨的任务之一是了解操作过程中可能发生的故障的性质（请参见侧栏“故障规划”）。 一旦了解了这些，就可以将缓解策略设计到软件中。

A failure’s cause is called a fault. A fault can be either internal or external to the system under consideration. Intermediate states between the occurrence of a fault and the occurrence of a failure are called errors. Faults can be prevented, tolerated, removed, or forecast. In this way a system becomes “resilient” to faults.

故障的原因称为故障。 故障可能是正在考虑的系统内部故障或外部故障。 故障发生与故障发生之间的中间状态称为错误。 可以预防，容忍，消除或预测故障。 这样，系统就可以“容忍”故障。

Among the areas with which we are concerned are how system faults are detected, how frequently system faults may occur, what happens when a fault occurs, how long a system is allowed to be out of operation, when faults or failures may occur safely, how faults or failures can be prevented, and what kinds of notifications are required when a failure occurs.

我们关注的领域包括如何检测系统故障，系统故障可能发生的频率，发生故障时发生的情况，允许系统停机多长时间，何时可以安全地发生故障或故障，如何进行检测。 可以防止出现故障或故障，并且可以防止发生故障时需要发出何种通知。

Because a system failure is observable by users, the time to repair is the time until the failure is no longer observable. This may be a brief delay in the response time or it may be the time it takes someone to fly to a remote location in the Andes to repair a piece of mining machinery (as was recounted to us by a person responsible for repairing the software in a mining machine engine). The notion of “observability” can be a tricky one: the Stuxnet virus, as an example, went unobserved for a very long time even though it was doing damage. In addition, we are often concerned with the level of capability that remains when a failure has occurred—a degraded operating mode.

由于用户可以观察到系统故障，因此修复时间就是直到不再可见故障为止的时间。 这可能是响应时间的短暂延迟，也可能是某人飞往安第斯山脉的偏远地点修理一台采矿机械的时间（正如负责维修该软件的人员向我们讲述的那样）。 采矿机引擎）。 “可观察性”的概念可能是一个棘手的概念：例如，Stuxnet病毒即使造成损害，也很长时间没有被观察到。 此外，我们经常担心故障发生时所保持的能力水平（运行模式降级）。

The distinction between faults and failures allows discussion of automatic repair strategies. That is, if code containing a fault is executed but the system is able to recover from the fault without any deviation from specified behavior being observable, there is no failure.

故障与故障之间的区别允许讨论自动修复策略。 即，如果执行了包含故障的代码，但是系统能够从故障中恢复，而不会观察到与指定行为的任何偏差，则不会出现故障。

The availability of a system can be calculated as the probability that it will provide the specified services within required bounds over a specified time interval. When referring to hardware, there is a well-known expression used to derive steady-state availability:

可以将系统的可用性计算为系统将在指定时间间隔内在要求范围内提供指定服务的概率。 当提到硬件时，有一个众所周知的表达式可用来导出稳态可用性：

$
\frac {MTBF} {MTBF + MTTR}
$

where MTBF refers to the mean time between failures and MTTR refers to the mean time to repair. In the software world, this formula should be interpreted to mean that when thinking about availability, you should think about what will make your system fail, how likely that is to occur, and that there will be some time required to repair it.

其中MTBF是指平均故障间隔时间，MTTR是指平均修复时间。 在软件世界中，应将此公式解释为意味着在考虑可用性时，您应考虑使系统发生故障的原因，发生故障的可能性以及修复该故障所需的时间。

From this formula it is possible to calculate probabilities and make claims like “99.999 percent availability,” or a 0.001 percent probability that the system will not be operational when needed. Scheduled downtimes (when the system is intentionally taken out of service) may not be considered when calculating availability, because the system is deemed “not needed” then; of course, this depends on the specific requirements for the system, often encoded in service-level agreements (SLAs). This arrangement may lead to seemingly odd situations where the system is down and users are waiting for it, but the downtime is scheduled and so is not counted against any availability requirements.

根据此公式，可以计算出概率，并做出诸如“ 99.999％的可用性”之类的索赔，或者提出系统在需要时无法运行的0.001％的概率。 在计算可用性时，可以不考虑计划内的停机时间（有意使系统退出服务时），因为那时系统被认为“不需要”。 当然，这取决于系统的特定要求，通常是在服务级别协议（SLA）中进行编码的。 这种安排可能会导致看似奇怪的情况，其中系统已关闭且用户正在等待它，但是停机时间是有计划的，因此不计入任何可用性要求。

In operational systems, faults are detected and correlated prior to being reported and repaired. Fault correlation logic will categorize a fault according to its severity (critical, major, or minor) and service impact (service-affecting or non-service-affecting) in order to provide the system operator with timely and accurate system status and allow for the appropriate repair strategy to be employed. The repair strategy may be automated or may require manual intervention.

在操作系统中，在报告和修复故障之前，先对其进行检测和关联。 故障关联逻辑将根据故障的严重性（严重，重大或次要）和服务影响（影响服务或不影响服务）对故障进行分类，以便为系统操作员提供及时，准确的系统状态并允许 采取适当的维修策略。 维修策略可能是自动化的，也可能需要手动干预。

The availability provided by a computer system or hosting service is frequently expressed as a service-level agreement. This SLA specifies the availability level that is guaranteed and, usually, the penalties that the computer system or hosting service will suffer if the SLA is violated. The SLA that Amazon provides for its EC2 cloud service is

计算机系统或托管服务提供的可用性通常表示为服务级别协议。 此SLA指定了可以保证的可用性级别，通常，如果违反了SLA，则计算机系统或托管服务将遭受的惩罚。 亚马逊为其EC2云服务提供的SLA是

> AWS will use commercially reasonable efforts to make Amazon EC2 available with an Annual Uptime Percentage [defined elsewhere] of at least 99.95% during the Service Year. In the event Amazon EC2 does not meet the Annual Uptime Percentage commitment, you will be eligible to receive a Service Credit as described below.
>
> AWS将通过商业上合理的努力来使Amazon EC2在服务年度内的年度正常运行率百分比（在其他地方定义）至少为99.95％。 如果Amazon EC2不符合年度正常运行时间百分比承诺，您将有资格获得如下所述的服务信用。

Table 5.1 provides examples of system availability requirements and associated threshold values for acceptable system downtime, measured over observation periods of 90 days and one year. The term high availability typically refers to designs targeting availability of 99.999 percent (“5 nines”) or greater. By definition or convention, only unscheduled outages contribute to system downtime.

表5.1提供了系统可用性要求的示例，以及在90天和一年的观察期内测得的可接受的系统停机相关阈值。 术语“高可用性”通常是指以99.999％（“ 5个9”）或更高的可用性为目标的设计。 根据定义或约定，只有计划外的停机会导致系统停机。

Table 5.1 System Availability Requirements 

Availability | Downtime/90 Days | Downtime/Year
--|--|--
99.0% | 21 hours, 36 minutes | 3 days, 15.6 hours
99.9% | 2 hours, 10 minutes | 8 hours, 0 minutes, 46 seconds
99.99% | 12 minutes, 58 seconds | 52 minutes, 34 seconds
99.999% | 1 minute, 18 seconds | 5 minutes, 15 seconds
99.9999% | 8 seconds | 32 seconds

表5.1系统可用性要求

可用性 | 宕机/90天 | 宕机/年
---|---|---
99.0% | 21 小时, 36 分 | 3 天, 15.6 小时
99.9% | 2 小时, 10 分 | 8 小时, 0 分, 46 秒
99.99% | 12 分, 58 秒 | 52 分, 34 秒
99.999% | 1 分, 18 秒 | 5 分, 15 秒
99.9999% | 8 秒 | 32 秒

> **Planning for Failure 计划失败**
>
> When designing a high-availability or safety-critical system, it’s tempting to say that failure is not an option. It’s a catchy phrase, but it’s a lousy design philosophy. In fact, failure is not only an option, it’s almost inevitable. What will make your system safe and available is planning for the occurrence of failure or (more likely) failures, and handling them with aplomb. The first step is to understand what kinds of failures your system is prone to, and what the consequences of each will be. Here are three well-known techniques for getting a handle on this.
>
> 在设计高可用性或安全性至关重要的系统时，很容易说失败不是一种选择。 这是一个吸引人的短语，但它是一个糟糕的设计理念。 实际上，失败不仅是一种选择，而且几乎是不可避免的。 使您的系统安全和可用的原因是计划发生故障或（更可能是）发生故障，并认真处理它们。 第一步是了解您的系统容易发生哪些类型的故障，以及每种故障的后果。 这是解决此问题的三种众所周知的技术。
>
> _Hazard analysis 危害分析_  
> Hazard analysis is a technique that attempts to catalog the hazards that can occur during the operation of a system. It categorizes each hazard according to its severity. For example, the DO-178B standard used in the aeronautics industry defines these failure condition levels in terms of their effects on the aircraft, crew, and passengers:
>
> 危害分析是一种试图对系统运行期间可能发生的危害进行分类的技术。 它根据危害的严重程度对其进行分类。 例如，航空工业中使用的DO-178B标准根据对飞机，机组人员和乘客的影响来定义以下故障状态级别：
> 
> * _Catastrophic_. This kind of failure may cause a crash. This failure represents the loss of critical function required to safely fly and land aircraft.
>   _灾难性的_。 这种故障可能会导致崩溃。 该故障表示安全飞行和降落飞机所需的关键功能丧失。
> * _Hazardous_. This kind of failure has a large negative impact on safety or performance, or reduces the ability of the crew to operate the aircraft due to physical distress or a higher workload, or causes serious or fatal injuries among the passengers.
>    _危险_。 这种故障会对安全或性能造成很大的负面影响，或者由于身体不适或工作量增加而降低机组人员操作飞机的能力，或者对乘客造成严重或致命的伤害。
> * _Major_. This kind of failure is significant, but has a lesser impact than a Hazardous failure (for example, leads to passenger discomfort rather than injuries) or significantly increases crew workload to the point where safety is affected.
>    _重大的_。 这种故障是重大的，但其影响要小于危险故障（例如，导致乘客不适而不是受伤），或者会大大增加机组人员的工作量，以至影响安全。
> * _Minor_. This kind of failure is noticeable, but has a lesser impact than a Major failure (for example, causing passenger inconvenience or a routine flight plan change).
>    _次要_。 这种故障很明显，但比重大故障（例如，造成乘客不便或更改常规航班计划）的影响要小。
> * _No effect_. This kind of failure has no impact on safety, aircraft operation, or crew workload.
>    _没有影响_。 这种故障不会影响安全性，飞机运行或机组人员的工作量。
>
> Other domains have their own categories and definitions. Hazard analysis also assesses the probability of each hazard occurring. Hazards for which the product of cost and probability exceed some threshold are then made the subject of mitigation activities.
>
> 其他域具有自己的类别和定义。 危害分析还评估了每种危害发生的可能性。 然后，将成本和概率乘积超过某个阈值的危害作为缓解活动的主题。
> 
> _Fault tree analysis_  
> _故障树分析_
> Fault tree analysis is an analytical technique that specifies a state of the system that negatively impacts safety or reliability, and then analyzes the system’s context and operation to find all the ways that the undesired state could occur. The technique uses a graphic construct (the fault tree) that helps identify all sequential and parallel sequences of contributing faults that will result in the occurrence of the undesired state, which is listed at the top of the tree (the “top event”). The contributing faults might be hardware failures, human errors, software errors, or any other pertinent events that can lead to the undesired state.
>
> 故障树分析是一种分析技术，它指定对安全性或可靠性有负面影响的系统状态，然后分析系统的上下文和操作以查找可能发生不良状态的所有方式。 该技术使用图形结构（故障树），该结构有助于识别所有导致故障的顺序和并行序列，这些故障将导致出现不希望的状态，该状态列在树的顶部（“顶部事件”）。 导致故障的原因可能是硬件故障，人为错误，软件错误或任何其他可能导致不良状态的相关事件。
> 
> Figure 5.1, taken from a NASA handbook on fault tree analysis, shows a very simple fault tree for which the top event is failure of component D. It shows that component D can fail if A fails and either B or C fails.
>
> 图5.1摘自NASA的故障树分析手册，该图显示了一个非常简单的故障树，其最主要的事件是组件D的故障。它表明，如果A发生故障并且B或C发生故障，则组件D可能发生故障。
> 
> The symbols that connect the events in a fault tree are called gate symbols, and are taken from Boolean logic diagrams. Figure 5.2 illustrates the notation.
>
> 连接故障树中事件的符号称为门符号，它们取自布尔逻辑图。 图5.2说明了该符号。
> 
> A fault tree lends itself to static analysis in various ways. For example, a “minimal cut set” is the smallest combination of events along the bottom of the tree that together can cause the top event. The set of minimal cut sets shows all the ways the bottom events can combine to cause the overarching failure. Any singleton minimal cut set reveals a single point of failure, which should be carefully scrutinized. Also, the probabilities of various contributing failures can be combined to come up with a probability of the top event occurring. Dynamic analysis occurs when the order of contributing failures matters. In this case, techniques such as Markov analysis can be used to calculate probability of failure over different failure sequences.
>
> 故障树可以通过各种方式进行静态分析。 例如，“最小割集”是沿着树底部的事件的最小组合，这些事件可以共同导致顶部事件。 最小割集集显示了底部事件可以组合导致总体失败的所有方式。 任何单例最小割集都揭示了单点故障，应仔细检查。 同样，可以将各种促成故障的概率进行组合，以得出发生顶级事件的概率。 当导致故障的顺序很重要时，就会进行动态分析。 在这种情况下，可以使用诸如马尔可夫分析之类的技术来计算不同故障序列上的故障概率。
> 
> Fault trees aid in system design, but they can also be used to diagnose failures at runtime. If the top event has occurred, then (assuming the fault tree model is complete) one or more of the contributing failures has occurred, and the fault tree can be used to track it down and initiate repairs.
>
> 故障树有助于系统设计，但是它们也可以用于在运行时诊断故障。 如果发生了最重要的事件，则（假设故障树模型已完成）发生了一个或多个导致故障的故障，并且可以使用故障树来对其进行跟踪并发起修复。
> 
> Failure Mode, Effects, and Criticality Analysis (FMECA) catalogs the kinds of failures that systems of a given type are prone to, along with how severe the effects of each one can be. FMECA relies on the history of
>
> 故障模式、影响和严重性分析（FMECA）列出了给定类型的系统容易发生的故障类型，以及每个系统可能造成的严重影响。 FMECA依赖于
> 
> ![Figure 5.1]()
> Figure 5.1 A simple fault tree. D fails if A fails and either B or C fails. 图5.1一个简单的故障树。 如果A失败并且B或C失败，则D失败。
> 
> failure of similar systems in the past. Table 5.2, also taken from the NASA handbook, shows the data for a system of redundant amplifiers. Historical data shows that amplifiers fail most often when there is a short circuit or the circuit is left open, but there are several other failure modes as well (lumped together as “Other”).
>
> 过去类似系统的故障。 表5.2（也取自NASA手册）显示了冗余放大器系统的数据。 历史数据表明，在发生短路或电路断开时，放大器最常发生故障，但是还有其他几种故障模式（统称为“其他”）。
> 
> ![]()
> Figure 5.2 Fault tree gate symbols 图5.2故障树门符号
> 
> ## Table 5.2 Failure Probabilities and Effects 表5.2故障概率和影响
> 
> Component | Failure Probability | Failure Mode | % Failures by Mode | Effects Critical | Effects Noncritical
> --|--|--|--|--|--
> A | 1 × 10^–3 | Open | 90 | | X
> _ |   | Short | 5 | X (5 × 10^–5) | 
> _ |   | Other | 5 | X (5 × 10^–5) |
> B | 1 × 10^–3 | Open | 90 | | X
> _ |   | Short | 5 | X (5 × 10^–5) | 
> _ |   | Other | 5 | X (5 × 10^–5) | 
>
> 组件 | 失败概率 | 故障模式 | % 模式故障 | 严重影响 | 非关键影响
> --|--|--|--|--|--
> A | 1 × 10^–3 | 开路 | 90 | | X
> _ |   | 短路 | 5 | X (5 × 10^–5) | 
> _ |   | 其他 | 5 | X (5 × 10^–5) |
> B | 1 × 10^–3 | 开路 | 90 | | X
> _ |   | 短路 | 5 | X (5 × 10^–5) | 
> _ |   | 其他 | 5 | X (5 × 10^–5) | 
>
> Adding up the critical column gives us the probability of a critical system failure: 5 × 10^–5 + 5 × 10^–5 + 5 × 10^–5 + 5 × 10^–5 = 2 × 10^–4. 
>
> 将关键列加起来可得出出现严重系统故障的可能性：5×10^–5 + 5×10^–5 + 5×10^–5 + 5×10^–5 = 2×10^–4 。
>
> These techniques, and others, are only as good as the knowledge and experience of the people who populate their respective data structures. One of the worst mistakes you can make, according to the NASA handbook, is to let form take priority over substance. That is, don’t let safety engineering become a matter of just filling out the tables. Instead, keep pressing to find out what else can go wrong, and then plan for it.
>
> 这些技术以及其他技术仅与填充各自数据结构的人员的知识和经验一样好。 根据NASA的手册，您可能犯的最严重的错误之一就是让形式优先于实质。 就是说，不要让安全工程成为只填写表格的问题。 相反，请继续寻找其他可能出问题的地方，然后进行计划。

### 5.1 Availability General Scenario 可用性通用场景

From these considerations we can now describe the individual portions of an availability general scenario. These are summarized in Table 5.3:

基于这些考虑，我们现在可以描述可用性一般方案的各个部分。 这些总结在表5.3中：

* _Source of stimulus_. We differentiate between internal and external origins of faults or failure because the desired system response may be different.
* _激励源_。 由于所需的系统响应可能会有所不同，因此我们将内部内部或外部故障或故障源区分开来。
* _Stimulus_. A fault of one of the following classes occurs:
* _激励_。 发生以下类别之一的故障：
  * _Omission_. A component fails to respond to an input.
  * _省略_。 组件无法响应输入。
  * _Crash_. The component repeatedly suffers omission faults.
  * _奔溃_。 该组件反复遭受遗漏故障。
  * _Timing_. A component responds but the response is early or late.
  * _时机_。 组件响应，但响应是早期或晚期。
  * _Response_. A component responds with an incorrect value.
  * _响应_。 组件响应的值不正确。
* _Artifact_. This specifies the resource that is required to be highly available, such as a processor, communication channel, process, or storage.
* _对象_。 这指定了高度可用的资源，例如处理器，通信通道，进程或存储。
* _Environment_. The state of the system when the fault or failure occurs may also affect the desired system response. For example, if the system has already seen some faults and is operating in other than normal mode, it may be desirable to shut it down totally. However, if this is the first fault observed, some degradation of response time or function may be preferred.
* _环境_。 发生故障或故障时的系统状态也可能影响所需的系统响应。 例如，如果系统已经出现一些故障并且以非正常模式运行，则可能需要将其完全关闭。 但是，如果这是观察到的第一个故障，则响应时间或功能的某些降低可能是优选的。
* _Response_. There are a number of possible reactions to a system fault. First, the fault must be detected and isolated (correlated) before any other response is possible. (One exception to this is when the fault is prevented before it occurs.) After the fault is detected, the system must recover from it. Actions associated with these possibilities include logging the failure, notifying selected users or other systems, taking actions to limit the damage caused by the fault, switching to a degraded mode with either less capacity or less function, shutting down external systems, or becoming unavailable during repair.
* _反应_。 对系统故障有许多可能的反应。 首先，必须先检测到故障并隔离（关联）故障，然后才能做出其他响应。 （对此的一个例外是在故障发生之前就已阻止了该故障。）检测到故障后，系统必须从中恢复。 与这些可能性相关的操作包括记录故障，通知选定的用户或其他系统，采取措施以限制由故障引起的损害，切换到容量或功能较少的降级模式，关闭外部系统或在此期间不可用 修理。
* _Response measure_. The response measure can specify an availability percentage, or it can specify a time to detect the fault, time to repair the fault, times or time intervals during which the system must be available, or the duration for which the system must be available.
* _反应测量_。 反应度量可以指定可用性百分比，也可以指定检测故障的时间，修复故障的时间，系统必须可用的时间或时间间隔，或系统必须可用的持续时间。

**Table 5.3** Availability General Scenario 可用性一般场景

Portion of Scenario | Possible Values
--|--
Source | Internal/external: people, hardware, software, physical infrastructure, physical environment
Stimulus | Fault: omission, crash, incorrect timing, incorrect response
Artifact | Processors, communication channels, persistent storage, processes
Environment | Normal operation, startup, shutdown, repair mode, degraded operation, overloaded operation
Response | Prevent the fault from becoming a failure<br>Detect the fault:<br><ul><li>Log the fault<li>Notify appropriate entities (people or systems)</ul>Recover from the fault:<ul><li>Disable source of events causing the fault<li>Be temporarily unavailable while repair is being effected<li>Fix or mask the fault/failure or contain the damage it causes<li>Operate in a degraded mode while repair is being effected</ul>
Response Measure | Time or time interval when the system must be available <br>Availability percentage (e.g., 99.999%) <br>Time to detect the fault  <br>Time to repair the fault <br>Time or time interval in which system can be in degraded mode Proportion (e.g., 99%) or rate (e.g., up to 100 per second) of a certain class of faults that the system prevents, or handles without failing

场景部分 | 可能的值
--|--
来源 | 内部/外部：人员，硬件，软件，物理基础结构，物理环境
刺激 | 故障：遗漏，崩溃，错误的时间，错误的响应
对象 | 处理器，通信通道，持久性存储，进程
环境 | 正常运行，启动，关闭，维修模式，降级运行，过载
反应 | 防止故障成为故障<br>检测故障：<br><li>记录故障<br><li>通知适当的实体（人员或系统）<br>从故障中恢复：<br><li>禁用导致故障的事件源<br><li>进行维修时暂时不可用<br><li>修复或掩盖故障/故障或包含其造成的损坏<br><li>在进行维修时以降级模式运行
反应测量 | 系统必须可用的时间或时间间隔<br>可用性百分比（例如99.999％）<br>检测故障的时间<br>修复故障的时间<br>系统可以处于的时间或时间间隔 降级模式系统防止或处理的某些故障类别的比例（例如99％）或比率（例如每秒100个）

Figure 5.3 shows a concrete scenario generated from the general scenario: The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime.

图5.3显示了从一般情况生成的具体情况：心跳监视器确定服务器在正常操作期间无响应。 该系统会通知操作员并继续运行而不会停机。

![]()
Figure 5.3 Sample concrete availability scenario

### 5.2 Tactics for Availability 可用性策略

A failure occurs when the system no longer delivers a service that is consistent with its specification; this failure is observable by the system’s actors. A fault (or combination of faults) has the potential to cause a failure. Availability tactics, therefore, are designed to enable a system to endure system faults so that a service being delivered by the system remains compliant with its specification. The tactics we discuss in this section will keep faults from becoming failures or at least bound the effects of the fault and make repair possible. We illustrate this approach in Figure 5.4.

当系统不再提供与其规格相一致的服务时，就会发生失效。 系统参与者可以观察到此失效。故障（或故障组合）有可能导致失效。 因此，可用性策略旨在使系统能够承受系统故障，从而使系统提供的服务保持符合其规范。 我们在本节中讨论的策略将防止故障成为失效，或者至少限制故障的影响，并使维修成为可能。 我们在图5.4中说明了这种方法。

Availability tactics may be categorized as addressing one of three categories: fault detection, fault recovery, and fault prevention. The tactics categorization for availability is shown in Figure 5.5 (on the next page). Note that it is often the case that these tactics will be provided for you by a software infrastructure, such as a middleware package, so your job as an architect is often one of choosing and assessing (rather than implementing) the right availability tactics and the right combination of tactics.

可用性策略可分类为解决以下三类之一：故障检测，故障恢复和故障预防。 可用性的策略分类如图5.5所示（下一页）。 请注意，通常这些策略是由软件基础结构（例如中间件软件包）为您提供的，因此，您作为架构师的工作通常是选择和评估（而不是实现）正确的可用性策略以及正确的战术组合。

![]()
**Figure 5.4** Goal of availability tactics 可用性策略的目标

![]()
**Figure 5.5** Availability tactics 可用性策略

**Detect Faults 检测故障**

Before any system can take action regarding a fault, the presence of the fault must be detected or anticipated. Tactics in this category include the following:

在任何系统可以针对故障采取措施之前，必须先检测到或预期到故障的存在。 此类别中的策略包括以下内容：

* _Ping/echo_ refers to an asynchronous request/response message pair exchanged between nodes, used to determine reachability and the round-trip delay through the associated network path. But the echo also determines that the pinged component is alive and responding correctly. The ping is often sent by a system monitor. Ping/echo requires a time threshold to be set; this threshold tells the pinging component how long to wait for the echo before considering the pinged component to have failed (“timed out”). Standard implementations of ping/echo are available for nodes interconnected via IP.
* _Ping/echo_ 是指在节点之间交换的异步请求/响应消息对，用于确定通过相关网络路径的可达性和往返延迟。但是，回声还确定ping的组件仍然有效并正确响应。 ping通常是由系统监视器发送的。 Ping/echo需要设置时间阈值； 此阈值告诉ping组件在考虑ping组件出现故障（“超时”）之前要等待回声多长时间。 ping / echo的标准实现可用于通过IP互连的节点。
* _Monitor_. A monitor is a component that is used to monitor the state of health of various other parts of the system: processors, processes, I/O, memory, and so on. A system monitor can detect failure or congestion in the network or other shared resources, such as from a denial-of-service attack. It orchestrates software using other tactics in this category to detect malfunctioning components. For example, the system monitor can initiate self-tests, or be the component that detects faulty time stamps or missed heartbeats. [1][1]
* _监视器_。监视器是用于监视系统其他各个部分的运行状况的组件：处理器，进程，I / O，内存等。 系统监视器可以检测网络或其他共享资源中的故障或拥塞，例如来自拒绝服务攻击。 它使用该类别中的其他策略来编排软件以检测故障组件。 例如，系统监视器可以启动自检，或者是检测错误时间戳或心跳缺失的组件。[1][1]
* _Heartbeat_ is a fault detection mechanism that employs a periodic message exchange between a system monitor and a process being monitored. A special case of heartbeat is when the process being monitored periodically resets the watchdog timer in its monitor to prevent it from expiring and thus signaling a fault. For systems where scalability is a concern, transport and processing overhead can be reduced by piggybacking heartbeat messages on to other control messages being exchanged between the process being monitored and the distributed system controller. The big difference between heartbeat and ping/echo is who holds the responsibility for initiating the health check—the monitor or the component itself.
* _心跳_ 是一种故障检测机制，在系统监视器和被监视的进程之间采用定期的消息交换。 心跳的一种特殊情况是，当被监视的进程定期重置其监视器中的看门狗计时器以防止其到期并因此发出故障信号时。 对于需要考虑可伸缩性的系统，可以通过将心跳消息附加到正在监视的进程和分布式系统控制器之间交换的其他控制消息上来减少传输和处理开销。 心跳和ping / echo之间的最大区别在于，谁负责启动运行状况检查（监视器或组件本身）。
* _Time stamp_. This tactic is used to detect incorrect sequences of events, primarily in distributed message-passing systems. A time stamp of an event can be established by assigning the state of a local clock to the event immediately after the event occurs. Simple sequence numbers can also be used for this purpose, if time information is not important.
* _时间戳_。该策略用于检测错误的事件序列，主要是在分布式消息传递系统中。 通过在事件发生后立即将本地时钟的状态分配给事件，可以建立事件的时间戳。 如果时间信息不重要，也可以将简单的序列号用于此目的。
* _Sanity checking_ checks the validity or reasonableness of specific operations or outputs of a component. This tactic is typically based on a knowledge of the internal design, the state of the system, or the nature of the information under scrutiny. It is most often employed at interfaces, to examine a specific information flow.
* _完整性检查_ 检查组件的特定操作或输出的有效性或合理性。该策略通常基于对内部设计，系统状态或所检查信息的性质的了解。它最常用于接口，以检查特定的信息流。
* _Condition monitoring_ involves checking conditions in a process or device, or validating assumptions made during the design. By monitoring conditions, this tactic prevents a system from producing faulty behavior. The computation of checksums is a common example of this tactic. However, the monitor must itself be simple (and, ideally, provable) to ensure that it does not introduce new software errors.
* _状态监视_ 涉及检查过程或设备中的条件，或验证设计过程中所做的假设。通过监视条件，此策略可防止系统产生错误的行为。校验和的计算是该策略的常见示例。但是，监视器本身必须很简单（理想情况下是可证明的），以确保它不会引入新的软件错误。
* _Voting_. The most common realization of this tactic is referred to as triple modular redundancy (TMR), which employs three components that do the same thing, each of which receives identical inputs, and forwards their output to voting logic, used to detect any inconsistency among the three output states. Faced with an inconsistency, the voter reports a fault. It must also decide what output to use. It can let the majority rule, or choose some computed average of the disparate outputs. This tactic depends critically on the voting logic, which is usually realized as a simple, rigorously reviewed and tested singleton so that the probability of error is low.
* _表决_。此策略最常见的实现方式是称为三重模块冗余（TMR），它采用三个执行相同操作的组件，每个组件接收相同的输入，并将其输出转发给投票逻辑，该逻辑用于检测主机之间的任何不一致。 三种输出状态。 面对不一致的情况，选民报告了错误。 它还必须决定使用什么输出。 它可以让大多数规则，或选择不同输出的一些计算平均值。 该策略严重依赖于表决逻辑，该逻辑通常实现为简单，经过严格审查和测试的单例，因此出错的可能性较低。
  * _Replication_ is the simplest form of voting; here, the components are exact clones of each other. Having multiple copies of identical components can be effective in protecting against random failures of hardware, but this cannot protect against design or implementation errors, in hardware or software, because there is no form of diversity embedded in this tactic.
  * _复制_ 是最简单的投票形式。 在这里，这些组件是彼此的精确克隆。 具有相同组件的多个副本可以有效地防止硬件的随机故障，但这不能防止硬件或软件中的设计或实现错误，因为此策略中没有嵌入任何形式的多样性。
  * _Functional redundancy_ is a form of voting intended to address the issue of common-mode failures (design or implementation faults) in hardware or software components. Here, the components must always give the same output given the same input, but they are diversely designed and diversely implemented.
  * _功能冗余_ 是一种投票形式，旨在解决硬件或软件组件中的共模故障（设计或实现故障）的问题。 在这里，组件必须始终在给定相同输入的情况下提供相同的输出，但是它们的设计和实现方式各不相同。
  * _Analytic redundancy_ permits not only diversity among components’ private sides, but also diversity among the components’ inputs and outputs. This tactic is intended to tolerate specification errors by using separate requirement specifications. In embedded systems, analytic redundancy also helps when some input sources are likely to be unavailable at times. For example, avionics programs have multiple ways to compute aircraft altitude, such as using barometric pressure, the radar altimeter, and geometrically using the straight-line distance and look-down angle of a point ahead on the ground. The voter mechanism used with analytic redundancy needs to be more sophisticated than just letting majority rule or computing a simple average. It may have to understand which sensors are currently reliable or not, and it may be asked to produce a higher-fidelity value than any individual component can, by blending and smoothing individual values over time.
  * _分析冗余_ 不仅允许组件私有侧之间的差异，还允许组件输入和输出之间的差异。 此策略旨在通过使用单独的需求规范来容忍规范错误。 在嵌入式系统中，当某些输入源有时可能不可用时，分析冗余也有帮助。 例如，航空电子程序有多种计算飞机高度的方法，例如使用大气压力，雷达高度计，以及在几何上使用地面上前方某个点的直线距离和俯视角度。 用于分析冗余的表决器机制需要更复杂，而不仅仅是让多数人统治或计算简单的平均值。 可能必须了解哪些传感器当前可靠或不可靠，并且可能需要通过随时间推移混合和平滑各个值来产生比任何单个组件都更高的保真度值。
* _Exception detection_ refers to the detection of a system condition that alters the normal flow of execution. The exception detection tactic can be further refined:
* _异常检测_ 是指检测会改变正常执行流程的系统条件。 异常检测策略可以进一步完善：
  * _System exceptions_ will vary according to the processor hardware architecture employed and include faults such as divide by zero, bus and address faults, illegal program instructions, and so forth.
  * _系统异常_ 将根据所采用的处理器硬件体系结构而变化，并且包括诸如除以零，总线和地址故障，非法程序指令等故障。
  * The _parameter fence_ tactic incorporates an a priori data pattern (such as 0xDEADBEEF) placed immediately after any variable-length parameters of an object. This allows for runtime detection of overwriting the memory allocated for the object’s variable-length parameters.
  * _参数围墙_ 策略将先验数据模式（例如0xDEADBEEF）并入对象的任何可变长度参数之后。 这样可以在运行时检测是否覆盖为对象的可变长度参数分配的内存。
  * _Parameter typing_ employs a base class that defines functions that add, find, and iterate over type-length-value (TLV) formatted message parameters. Derived classes use the base class functions to implement functions that provide parameter typing according to each parameter’s structure. Use of strong typing to build and parse messages results in higher availability than implementations that simply treat messages as byte buckets. Of course, all design involves tradeoffs. When you employ strong typing, you typically trade higher availability against ease of evolution.
  * _参数类型化_ 使用一个基类，该基类定义了一些函数，这些函数添加，查找和迭代类型长度值（TLV）格式的消息参数。 派生类使用基类函数来实现根据每个参数的结构提供参数类型的函数。 与仅将消息视为字节存储区的实现相比，使用强类型来构建和解析消息可提高可用性。 当然，所有设计都需要权衡。 当您使用强类型时，通常会以更高的可用性与易用性进行交易。
  * _Timeout_ is a tactic that raises an exception when a component detects that it or another component has failed to meet its timing constraints. For example, a component awaiting a response from another component can raise an exception if the wait time exceeds a certain value.
  * _超时_ 是一种策略，当一个组件检测到它或另一个组件未能满足其时序约束时会引发异常。 例如，如果等待时间超过某个值，则等待另一个组件响应的组件可能会引发异常。
* _Self-test_. Components (or, more likely, whole subsystems) can run procedures to test themselves for correct operation. Self-test procedures can be initiated by the component itself, or invoked from time to time by a system monitor. These may involve employing some of the techniques found in condition monitoring, such as checksums.
* _自检_。组件（或更可能是整个子系统）可以运行过程以测试自身是否正确运行。 自检过程可以由组件本身启动，也可以由系统监视器不时调用。 这些可能涉及采用状态监视中发现的某些技术，例如校验和。

**Recover from Faults 故障恢复**

Recover-from-faults tactics are refined into preparation-and-repair tactics and reintroduction tactics. The latter are concerned with reintroducing a failed (but rehabilitated) component back into normal operation. Preparation-and-repair tactics are based on a variety of combinations of retrying a computation or introducing redundancy. They include the following:

“从故障中恢复”策略被细化为“准备和修复”策略和“重新引入”策略。 后者与将发生故障（但已修复）的组件重新引入正常操作有关。 准备和修复策略基于重试计算或引入冗余的多种组合。 其中包括：

* _Active redundancy (hot spare)_. This refers to a configuration where all of the nodes (active or redundant spare) in a protection group[2][2] receive and process identical inputs in parallel, allowing the redundant spare(s) to maintain synchronous state with the active node(s). Because the redundant spare possesses an identical state to the active processor, it can take over from a failed component in a matter of milliseconds. The simple case of one active node and one redundant spare node is commonly referred to as 1+1 (“one plus one”) redundancy. Active redundancy can also be used for facilities protection, where active and standby network links are used to ensure highly available network connectivity.
* _主动冗余（热备）_。这是指一种配置，其中保护组[2][2]中的所有节点（活动或冗余备用节点）并行接收和处理相同的输入，从而允许冗余备用节点与活动节点保持同步状态（ s）。 因为冗余备件具有与活动处理器相同的状态，所以它可以在几毫秒内从发生故障的组件中接管。 一个活动节点和一个冗余备用节点的简单情况通常称为1+1（“一加一”）冗余。 活动冗余也可以用于设施保护，其中活动和备用网络链接用于确保高可用性的网络连接。
* _Passive redundancy (warm spare)_. This refers to a configuration where only the active members of the protection group process input traffic; one of their duties is to provide the redundant spare(s) with periodic state updates. Because the state maintained by the redundant spares is only loosely coupled with that of the active node(s) in the protection group (with the looseness of the coupling being a function of the checkpointing mechanism employed between active and redundant nodes), the redundant nodes are referred to as warm spares. Depending on a system’s availability requirements, passive redundancy provides a solution that achieves a balance between the more highly available but more compute-intensive (and expensive) active redundancy tactic and the less available but significantly less complex cold spare tactic (which is also significantly cheaper). (For an example of implementing passive redundancy, see the section on code templates in Chapter 19.)
* _被动冗余（温备）_。 这是指仅保护组的活动成员处理输入流量的配置； 他们的职责之一是为冗余备件提供定期的状态更新。 由于冗余备件维护的状态仅与保护组中活动节点的状态松散耦合（耦合的松散度取决于活动节点和冗余节点之间使用的检查点机制），因此冗余节点 被称为温暖备件。 根据系统的可用性要求，被动冗余提供了一种解决方案，可在较高可用性，但计算量更大（且价格昂贵）的主动冗余策略与较少可用性但复杂程度较低的冷备用策略（也显着便宜）之间取得平衡 ）。 （有关实现被动冗余的示例，请参见第19章中的代码模板部分。）
* _Spare (cold spare)_. Cold sparing refers to a configuration where the redundant spares of a protection group remain out of service until a fail-over occurs, at which point a power-on-reset procedure is initiated on the redundant spare prior to its being placed in service. Due to its poor recovery performance, cold sparing is better suited for systems having only high-reliability (MTBF) requirements as opposed to those also having high-availability requirements.
* _备用（冷备）_。 冷备用是指一种配置，在该配置中，保护组的冗余备件将保持不可用状态，直到发生故障转移为止，此时，将在冗余备件投入使用之前对其启动上电复位过程。 由于恢复性能差，冷备用更适合仅具有高可靠性（MTBF）要求的系统，而不是也具有高可用性要求的系统。
* _Exception handling_. Once an exception has been detected, the system must handle it in some fashion. The easiest thing it can do is simply to crash, but of course that’s a terrible idea from the point of availability, usability, testability, and plain good sense. There are much more productive possibilities. The mechanism employed for exception handling depends largely on the programming environment employed, ranging from simple function return codes (error codes) to the use of exception classes that contain information helpful in fault correlation, such as the name of the exception thrown, the origin of the exception, and the cause of the exception thrown. Software can then use this information to mask the fault, usually by correcting the cause of the exception and retrying the operation.
* _异常处理_。 一旦检测到异常，系统必须以某种方式处理它。 它可以做的最简单的事情就是崩溃，但是从可用性，可使用性，可测试性和通俗易懂的角度来看，这当然是一个糟糕的主意。 还有更多的生产可能性。 异常处理所采用的机制在很大程度上取决于所采用的编程环境，范围从简单的函数返回码（错误代码）到异常类的使用，这些异常类包含有助于故障相关的信息，例如抛出的异常的名称，异常的来源 异常以及引发异常的原因。 然后，软件通常可以通过更正异常原因并重试操作，使用此信息来掩盖故障。
* _Rollback_. This tactic permits the system to revert to a previous known good state, referred to as the “rollback line”—rolling back time—upon the detection of a failure. Once the good state is reached, then execution can continue. This tactic is often combined with active or passive redundancy tactics so that after a rollback has occurred, a standby version of the failed component is promoted to active status. Rollback depends on a copy of a previous good state (a checkpoint) being available to the components that are rolling back. Checkpoints can be stored in a fixed location and updated at regular intervals, or at convenient or significant times in the processing, such as at the completion of a complex operation.
* _回滚_。 此策略允许系统在检测到故障后恢复到先前已知的良好状态，称为“回退线”（回退时间）。 一旦达到良好状态，就可以继续执行。 此策略通常与主动或被动冗余策略结合使用，以便在发生回滚之后，将故障组件的备用版本提升为活动状态。 回滚取决于先前良好状态（检查点）的副本可用于正在回滚的组件。 可以将检查点存储在固定位置，并在处理过程中（例如在完成复杂操作时）定期或在方便或重要的时间进行更新。
* _Software upgrade_ is another preparation-and-repair tactic whose goal is to achieve in-service upgrades to executable code images in a non-service-affecting manner. This may be realized as a function patch, a class patch, or a hitless in-service software upgrade (ISSU). A function patch is used in procedural programming and employs an incremental linker/loader to store an updated software function into a pre-allocated segment of target memory. The new version of the software function will employ the entry and exit points of the deprecated function. Also, upon loading the new software function, the symbol table must be updated and the instruction cache invalidated. The class patch tactic is applicable for targets executing object-oriented code, where the class definitions include a back-door mechanism that enables the runtime addition of member data and functions. Hitless in-service software upgrade leverages the active redundancy or passive redundancy tactics to achieve non-service-affecting upgrades to software and associated schema. In practice, the function patch and class patch are used to deliver bug fixes, while the hitless in-service software upgrade is used to deliver new features and capabilities.
* _软件升级_ 是另一种准备和修复策略，其目标是以不影响服务的方式实现对可执行代码映像的服务中升级。这可以通过功能补丁，类补丁或无中断运行中软件升级（ISSU）来实现。功能补丁用于过程编程中，并使用增量链接器/加载器将更新的软件功能存储到目标内存的预分配段中。新版本的软件功能将使用不赞成使用的功能的入口和出口。同样，在加载新软件功能时，必须更新符号表，并使指令高速缓存无效。类补丁策略适用于执行面向对象代码的目标，其中类定义包括后门机制，该机制可在运行时添加成员数据和函数。无中断服务中软件升级利用主动冗余或被动冗余策略来实现对软件和相关架构的不影响服务的升级。实际上，功能补丁和类补丁用于提供错误修复，而无中断的在线服务软件升级用于提供新功能。
* _Retry_. The retry tactic assumes that the fault that caused a failure is transient and retrying the operation may lead to success. This tactic is used in networks and in server farms where failures are expected and common. There should be a limit on the number of retries that are attempted before a permanent failure is declared.
* _重试_。 重试策略假定导致故障的故障是暂时的，重试操作可能会导致成功。 在网络和服务器场中，这种策略是经常发生并经常发生故障的策略。 在声明永久性故障之前，尝试重试的次数应该有限制。
* _Ignore faulty behavior_. This tactic calls for ignoring messages sent from a particular source when we determine that those messages are spurious. For example, we would like to ignore the messages of an external component launching a denial-of-service attack by establishing Access Control List filters, for example.
* _忽略错误的行为_。 当我们确定这些消息是伪造的时，此策略要求忽略从特定来源发送的消息。 例如，我们想通过建立访问控制列表过滤器来忽略外部组件发起拒绝服务攻击的消息。
* The _degradation_ tactic maintains the most critical system functions in the presence of component failures, dropping less critical functions. This is done in circumstances where individual component failures gracefully reduce system functionality rather than causing a complete system failure.
* 在存在组件故障的情况下，_降级策略_ 可维持最关键的系统功能，而降低次要功能。 在单个组件故障会适当减少系统功能而不是导致整个系统故障的情况下，可以这样做。
* _Reconfiguration_ attempts to recover from component failures by reassigning responsibilities to the (potentially restricted) resources left functioning, while maintaining as much functionality as possible.
* _重新配置_ 尝试通过将职责重新分配给（可能受限制的）剩余功能来恢复组件故障，同时保持尽可能多的功能。

Reintroduction is where a failed component is reintroduced after it has been corrected. Reintroduction tactics include the following:

重新引入是在纠正故障组件后将其重新引入的地方。重新引入策略包括以下内容：

* The _shadow_ tactic refers to operating a previously failed or in-service upgraded component in a “shadow mode” for a predefined duration of time prior to reverting the component back to an active role. During this duration its behavior can be monitored for correctness and it can repopulate its state incrementally.
* _影子_ 策略是指在将组件恢复为活动角色之前，以“影子模式”将先前发生故障或服务中的升级组件运行预定的持续时间。 在此期间，可以监视其行为的正确性，并可以递增其状态。
* _State resynchronization_ is a reintroduction partner to the active redundancy and passive redundancy preparation-and-repair tactics. When used alongside the active redundancy tactic, the state resynchronization occurs organically, because the active and standby components each receive and process identical inputs in parallel. In practice, the states of the active and standby components are periodically compared to ensure synchronization. This comparison may be based on a cyclic redundancy check calculation (checksum) or, for systems providing safety-critical services, a message digest calculation (a one-way hash function). When used alongside the passive redundancy (warm spare) tactic, state resynchronization is based solely on periodic state information transmitted from the active component(s) to the standby component(s), typically via checkpointing. A special case of this tactic is found in stateless services, whereby any resource can handle a request from another (failed) resource.
* _状态重新同步_ 是主动冗余和被动冗余准备与修复策略的重新引入伙伴。 与主动冗余策略一起使用时，状态重新同步自然发生，因为主动和备用组件各自并行接收和处理相同的输入。 实际上，将定期比较活动组件和备用组件的状态以确保同步。 该比较可以基于循环冗余校验计算（校验和），或者对于提供安全关键服务的系统，基于消息摘要计算（单向哈希函数）。 当与被动冗余（热备用）策略一起使用时，状态重新同步通常仅基于从活动组件传输到备用组件（通常通过检查点）的周期性状态信息。 这种策略的一个特例是在无状态服务中发现，其中任何资源都可以处理来自另一个（失败）资源的请求。
* _Escalating restart_ is a reintroduction tactic that allows the system to recover from faults by varying the granularity of the component(s) restarted and minimizing the level of service affected. For example, consider a system that supports four levels of restart, as follows. The lowest level of restart (call it Level 0), and hence having the least impact on services, employs passive redundancy (warm spare), where all child threads of the faulty component are killed and recreated. In this way, only data associated with the child threads is freed and reinitialized. The next level of restart (Level 1) frees and reinitializes all unprotected memory (protected memory would remain untouched). The next level of restart (Level 2) frees and reinitializes all memory, both protected and unprotected, forcing all applications to reload and reinitialize. And the final level of restart (Level 3) would involve completely reloading and reinitializing the executable image and associated data segments. Support for the escalating restart tactic is particularly useful for the concept of graceful degradation, where a system is able to degrade the services it provides while maintaining support for mission-critical or safety-critical applications.
* _升级重启_ 是一种重新引入的策略，通过改变重启组件的粒度并最小化受影响的服务水平，允许系统从故障中恢复。例如，考虑如下支持四个重启级别的系统。最低级别的重新启动（称为级别0），因此对服务的影响最小，采用被动冗余（热备用），其中，故障组件的所有子线程都将被杀死并重新创建。这样，仅释放和重新初始化与子线程关联的数据。下一级别的重新启动（级别1）将释放并重新初始化所有不受保护的内存（受保护的内存将保持不变）。下一级别的重新启动（级别2）将释放并重新初始化所有受保护和不受保护的内存，从而迫使所有应用程序重新加载和重新初始化。重新启动的最终级别（级别3）将涉及完全重新加载并重新初始化可执行映像和关联的数据段。对逐步重启策略的支持对于平稳降级的概念特别有用，在这种降级中，系统能够降级其提供的服务，同时保持对关键任务或安全关键应用程序的支持。
* _Non-stop forwarding_ (NSF) is a concept that originated in router design. In this design functionality is split into two parts: supervisory, or control plane (which manages connectivity and routing information), and data plane (which does the actual work of routing packets from sender to receiver). If a router experiences the failure of an active supervisor, it can continue  forwarding packets along known routes—with neighboring routers—while the routing protocol information is recovered and validated. When the control plane is restarted, it implements what is sometimes called “graceful restart,” incrementally rebuilding its routing protocol database even as the data plane continues to operate.
* _不间断转发_（NSF）是源自路由器设计的概念。 在此设计中，功能分为两部分：管理或控制平面（管理连接性和路由信息），以及数据平面（完成将数据包从发送方路由到接收方的实际工作）。 如果路由器遇到活动的管理器故障，则它可以继续与相邻路由器一起沿着已知路由转发数据包，同时恢复并验证路由协议信息。 重新启动控制平面后，即使数据平面继续运行，它也会执行有时称为“正常重启”的操作，从而逐步重建其路由协议数据库。

**Prevent Faults 预防故障**

Instead of detecting faults and then trying to recover from them, what if your system could prevent them from occurring in the first place? Although this sounds like some measure of clairvoyance might be required, it turns out that in many cases it is possible to do just that. [3][3]

除了检测故障然后尝试从故障中恢复之外，如果您的系统可以从一开始就防止它们发生怎么办？ 尽管这听起来可能需要一些千里眼，但事实证明，在许多情况下，可以做到这一点。 [3][3]

* _Removal from service_. This tactic refers to temporarily placing a system component in an out-of-service state for the purpose of mitigating potential system failures. One example involves taking a component of a system out of service and resetting the component in order to scrub latent faults (such as memory leaks, fragmentation, or soft errors in an unprotected cache) before the accumulation of faults affects service (resulting in system failure). Another term for this tactic is software rejuvenation.
* _从服务中删除_。 此策略是指为了减轻潜在的系统失效，将系统组件暂时置于服务停止状态。 一个示例涉及使系统的某个组件停止服务并重置该组件，以便在故障累积影响服务（导致系统故障）之前清除潜在故障（例如内存泄漏，碎片或不受保护的缓存中的软错误）。 ）。 此策略的另一个术语是软件复原。
* _Transactions_. Systems targeting high-availability services leverage transactional semantics to ensure that asynchronous messages exchanged between distributed components are atomic, consistent, isolated, and durable. These four properties are called the “ACID properties.” The most common realization of the transactions tactic is “two-phase commit” (a.k.a. 2PC) protocol. This tactic prevents race conditions caused by two processes attempting to update the same data item.
* _事务_。面向高可用性服务的系统利用事务语义来确保分布式组件之间交换的异步消息是原子的，一致的，隔离的和持久的。 这四个属性称为“ ACID属性”。事务策略的最常见实现是“两阶段提交”（也称为2PC）协议。 此策略可防止由两个试图更新同一数据项的进程引起的争用条件。
* _Predictive model_. A predictive model, when combined with a monitor, is employed to monitor the state of health of a system process to ensure that the system is operating within its nominal operating parameters, and to take corrective action when conditions are detected that are predictive of likely future faults. The operational performance metrics monitored are used to predict the onset of faults; examples include session establishment rate (in an HTTP server), threshold crossing (monitoring high and low water marks for some constrained, shared resource), or maintaining statistics for process state (in service, out of service, under maintenance, idle), message queue length statistics, and so on.
* _预测模型_。 当与监控器结合使用预测模型时，该模型将用于监视系统进程的运行状况，以确保系统在其正常运行参数范围内运行，并在检测到可预测未来的条件时采取纠正措施 故障。 监视的操作性能指标用于预测故障的发生； 示例包括会话建立速率（在HTTP服务器中），阈值穿越（监视某些受限共享资源的高水位线和低水位线）或维护进程状态的统计信息（服务中，服务中，维护中，空闲），消息 队列长度统计信息，等等。
* _Exception prevention_. This tactic refers to techniques employed for the purpose of preventing system exceptions from occurring. The use of exception classes, which allows a system to transparently recover from system exceptions, was discussed previously. Other examples of exception prevention include abstract data types, such as smart pointers, and the use of wrappers to prevent faults, such as dangling pointers and semaphore access violations from occurring. Smart pointers prevent exceptions by doing bounds checking on pointers, and by ensuring that resources are automatically deallocated when no data refers to it. In this way resource leaks are avoided.
* _异常预防_。 该策略是指用于防止发生系统异常的技术。 前面已经讨论了使用异常类，该类允许系统透明地从系统异常中恢复。 异常预防的其他示例包括抽象数据类型（例如智能指针）以及使用包装程序来防止错误（例如悬挂的指针和信号量访问冲突）发生。 智能指针通过对指针进行边界检查以及确保在没有数据引用时自动释放资源来防止异常。 这样，避免了资源泄漏。
* _Increase competence set_. A program’s competence set is the set of states in which it is “competent” to operate. For example, the state when the denominator is zero is outside the competence set of most divide programs. When a component raises an exception, it is signaling that it has discovered itself to be outside its competence set; in essence, it doesn’t know what to do and is throwing in the towel. Increasing a component’s competence set means designing it to handle more cases—faults—as part of its normal operation. For example, a component that assumes it has access to a shared resource might throw an exception if it discovers that access is blocked. Another component might simply wait for access, or return immediately with an indication that it will complete its operation on its own the next time it does have access. In this example, the second component has a larger competence set than the first.
* _增加能力设定_。 程序的权限集是在其中“有能力”运行的状态集。 例如，分母为零时的状态在大多数除法程序的能力范围之外。 当某个组件引发异常时，它表示它已发现自己不在其能力集中。 从本质上讲，它不知道该怎么办，并且正在全力以赴。 增加组件的能力集意味着将其设计为处理更多情况（故障），这是其正常运行的一部分。 例如，假设某个组件有权访问共享资源，则该组件如果发现访问被阻止，可能会引发异常。 另一个组件可能只是在等待访问，或者立即返回并指示下一次访问时将自行完成操作。 在此示例中，第二个组件的能力集比第一个更大。

[1]: When the detection mechanism is implemented using a counter or timer that is periodically reset, this specialization of system monitor is referred to as a “watchdog.” During nominal operation, the process being monitored will periodically reset the watchdog counter/timer as part of its signal that it’s working correctly; this is sometimes referred to as “petting the watchdog.” 当使用定期重置的计数器或计时器来实现检测机制时，系统监视器的这种特殊性被称为“看门狗”。在正常运行期间，被监视的过程将定期重置看门狗计数器/定时器作为其一部分。 表示其工作正常； 有时称为“喂狗”。

[2]: A protection group is a group of processing nodes where one or more nodes are “active,” with the remaining nodes in the protection group serving as redundant spares. 保护组是一组处理节点，其中一个或多个节点处于“活动”状态，而保护组中的其余节点用作冗余备件。

[3]: These tactics deal with runtime means to prevent faults from occurring. Of course, an excellent way to prevent faults—at least in the system you’re building, if not in systems that your system must interact with—is to produce high-quality code. This can be done by means of code inspections, pair programming, solid requirements reviews, and a host of other good engineering practices. 这些策略涉及运行时方法，以防止发生故障。 当然，至少在正在构建的系统中（如果不是在必须与之交互的系统中）防止错误的一种极好的方法是生成高质量的代码。 这可以通过代码检查，结对编程，可靠的需求检查以及许多其他良好的工程实践来完成。

### 5.3 A Design Checklist for Availability 可用性设计检查表

Table 5.4 is a checklist to support the design and analysis process for availability.

表5.4是一个清单，以支持可用性的设计和分析过程。

**Table 5.4** Checklist to Support the Design and Analysis Process for Availability 支持可用性的设计和分析过程的清单

Category | Checklist
--|--
Allocation of Responsibilities | Determine the system responsibilities that need to be highly available. Within those responsibilities, ensure that additional responsibilities have been allocated to detect an omission, crash, incorrect timing, or incorrect response. Additionally, ensure that there are responsibilities to do the following: <br> <li> Log the fault <br> <li> Notify appropriate entities (people or systems) <br> <li> Disable the source of events causing the fault <br> <li> Be temporarily unavailable <br> <li> Fix or mask the fault/failure <br> <li> Operate in a degraded mode
Coordination Model | Determine the system responsibilities that need to be highly available. With respect to those responsibilities, do the following: <br> <li> Ensure that coordination mechanisms can detect an omission, crash, incorrect timing, or incorrect response. Consider, for example, whether guaranteed delivery is necessary. Will the coordination work under conditions of degraded communication? <br> <li> Ensure that coordination mechanisms enable the logging of the fault, notification of appropriate entities, disabling of the source of the events causing the fault, fixing or masking the fault, or operating in a degraded mode. <br> <li> Ensure that the coordination model supports the replacement of the artifacts used (processors, communications channels, persistent storage, and processes). For example, does replacement of a server allow the system to continue to operate? <br> <li> Determine if the coordination will work under conditions of degraded communication, at startup/shutdown, in repair mode, or under overloaded operation. For example, how much lost information can the coordination model withstand and with what consequences?
Data Model | Determine which portions of the system need to be highly available. Within those portions, determine which data abstractions, along with their operations or their properties, could cause a fault of omission, a crash, incorrect timing behavior, or an incorrect response. <br> For those data abstractions, operations, and properties, ensure that they can be disabled, be temporarily unavailable, or be fixed or masked in the event of a fault. <br> For example, ensure that write requests are cached if a server is temporarily unavailable and performed when the server is returned to service.
Mapping among Architectural Elements | Determine which artifacts (processors, communication channels, persistent storage, or processes) may produce a fault: omission, crash, incorrect timing, or incorrect response. <br> Ensure that the mapping (or remapping) of architectural elements is flexible enough to permit the recovery from the fault. This may involve a consideration of the following:  <br> <li> Which processes on failed processors need to be reassigned at runtime <br> <li> Which processors, data stores, or communication channels can be activated or reassigned at runtime <br> <li> How data on failed processors or storage can be served by replacement units <br> <li> How quickly the system can be reinstalled based on the units of delivery provided <br> <li> How to (re)assign runtime elements to processors, communication channels, and data stores  <br> <li> When employing tactics that depend on redundancy of functionality, the mapping from modules to redundant components is important. For example, it is possible to write one module that contains code appropriate for both the active component and backup components in a protection group.
Resource Management | Determine what critical resources are necessary to continue operating in the presence of a fault: omission, crash, incorrect timing, or incorrect response. Ensure there are sufficient remaining resources in the event of a fault to log the fault; notify appropriate entities (people or systems); disable the source of events causing the fault; be temporarily unavailable; fix or mask the fault/failure; operate normally, in startup, shutdown, repair mode, degraded operation, and overloaded operation. <br> Determine the availability time for critical resources, what critical resources must be available during specified time intervals, time intervals during which the critical resources may be in a degraded mode, and repair time for critical resources. Ensure that the critical resources are available during these time intervals. <br> For example, ensure that input queues are large enough to buffer anticipated messages if a server fails so that the messages are not permanently lost.

Table 5.4 Checklist to Support the Design and Analysis Process for
Availability, continued

Category | Checklist
--|--
Binding Time | Determine how and when architectural elements are bound. If late binding is used to alternate between components that can themselves be sources of faults (e.g., processes, processors, communication channels), ensure the chosen availability strategy is sufficient to cover faults introduced by all sources. For example: <br> <li> If late binding is used to switch between artifacts such as processors that will receive or be the subject of faults, will the chosen fault detection and recovery mechanisms work for all possible bindings? <br> <li> If late binding is used to change the definition or tolerance of what constitutes a fault (e.g., how long a process can go without responding before a fault is assumed), is the recovery strategy chosen sufficient to handle all cases? For example, if a fault is flagged after 0.1 milliseconds, but the recovery mechanism takes 1.5 seconds to work, that might be an unacceptable mismatch. <br> <li> What are the availability characteristics of the late binding mechanism itself? Can it fail? 
Choice of Technology | Determine the available technologies that can (help) detect faults, recover from faults, or reintroduce failed components. <br> Determine what technologies are available that help the response to a fault (e.g., event loggers). <br> Determine the availability characteristics of chosen technologies themselves: What faults can they recover from? What faults might they introduce into the system?

类别 | Checklist
--|--
职责分配 | 确定需要高度可用的系统职责。 在这些职责中，请确保已分配其他职责以检测遗漏，崩溃，错误的时间或错误的响应。 此外，请确保有责任执行以下操作：<li>记录故障<br> <li>通知适当的实体（人员或系统）<br> <li>禁用导致故障的事件源<br> <li>暂时无法使用<br> <li> 修复或掩盖故障/失败<br> <li>在降级模式下操作
协调模型 | 确定需要高度可用的系统职责。关于这些责任，请执行以下操作：<br><li>确保协调机制可以检测到遗漏，崩溃，错误的时间安排或错误的响应。例如，考虑是否需要保证交付。协调将在通讯质量下降的情况下进行吗？<br> <li>确保协调机制启用故障记录，通知的适当实体，禁用导致故障的事件源，修复或掩盖故障或以降级模式运行。<br> <li>确保协调模型支持替换所使用的构件（处理器，通信通道，持久性存储和进程）。例如，更换服务器是否允许系统继续运行？<br> <li>确定协调在通信质量下降，启动/关闭，维修模式还是过载的情况下是否可以工作。例如，协调模型可以承受多少丢失的信息，其后果是什么？
数据模型 | 确定系统的哪些部分需要高度可用。 在这些部分中，确定哪些数据抽象及其操作或属性可能导致遗漏故障，崩溃，错误的计时行为或错误的响应。<br> 对于那些数据抽象，操作和属性，请确保它们可以被禁用，暂时不可用，或者在发生故障时被修复或屏蔽。<br>例如，如果服务器暂时不可用并在服务器恢复服务时执行，请确保写入请求被缓存。
架构元素之间的映射 | 确定哪些工件（处理器，通信通道，持久性存储或进程）可能产生故障：遗漏，崩溃，错误的计时或错误的响应。<br>确保架构元素的映射（或重新映射）足够灵活以允许从故障中恢复。 这可能涉及以下方面的考虑：<br><li>故障处理器上的哪些进程需要在运行时重新分配<br> <li>在运行时可以激活或重新分配哪些处理器，数据存储或通信通道<br> <li>更换单元如何处理故障处理器或存储上的数据<br> <li>根据提供的交付单位，可以多快的时间重新安装系统<br> <li>如何将运行时元素（重新）分配给处理器，进行通讯渠道和数据存储<br> <li>当采用依赖于功能冗余的策略时，从模块到冗余组件的映射很重要。 例如，可以编写一个模块，其中包含适用于保护组中活动组件和备用组件的代码。
资源管理 | 确定在出现故障时继续运行所需的关键资源：遗漏，崩溃，错误的时间或错误的响应。 确保在发生故障时有足够的剩余资源来记录故障； 通知适当的实体（人员或系统）； 禁用导致故障的事件源； 暂时不可用； 修复或掩盖故障/失败； 在启动，关闭，修复模式，降级操作和过载操作中正常运行。<br>确定关键资源的可用性时间，在指定的时间间隔内必须有哪些关键资源，关键资源可能处于降级模式的时间间隔以及关键资源的修复时间。 确保在这些时间间隔内关键资源可用。<br>例如，确保输入队列足够大，以便在服务器发生故障时缓冲预期的消息，以使消息不会永久丢失。
绑定时间 | 确定如何以及何时绑定架构元素。 如果使用后期绑定在本身可能是故障源的组件（例如流程，处理器，通信通道）之间进行切换，请确保所选的可用性策略足以覆盖所有故障源所引入的故障。 例如：<br><li>如果使用后期绑定在工件（例如将要接收或成为故障对象的处理器）之间进行切换，那么所选的故障检测和恢复机制是否适用于所有可能的绑定？<br> <li>如果使用后期绑定来更改故障原因的定义或容忍度（例如，在假定故障之前流程可以进行多长时间而没有响应），那么选择的恢复策略是否足以处理所有情况？ 例如，如果在0.1毫秒后标记了一个故障，但是恢复机制需要1.5秒钟才能起作用，那么这可能是不可接受的不匹配。<br> <li>后期绑定机制本身的可用性特征是什么？ 会失败吗？  
技术选择 | 确定可以（帮助）检测故障，从故障中恢复或重新引入故障组件的可用技术。<br> 确定哪些可用的技术可以帮助对故障做出响应（例如，事件记录器）。<br>确定所选技术本身的可用性特征：它们可以从哪些故障中恢复？ 他们可能将哪些故障引入系统？

### 5.4 Summary 小结

Availability refers to the ability of the system to be available for use, especially after a fault occurs. The fault must be recognized (or prevented) and then the system must respond in some fashion. The response desired will depend on the criticality of the application and the type of fault and can range from “ignore it” to “keep on going as if it didn’t occur.”

可用性是指系统可使用的能力，尤其是在发生故障之后。 必须识别（或预防）故障，然后系统必须以某种方式进行响应。 所需的响应取决于应用程序的关键程度和故障的类型，范围从“忽略”到“继续进行，好像没有发生”。

Tactics for availability are categorized into detect faults, recover from faults and prevent faults. Detection tactics depend, essentially, on detecting signs of life from various components. Recovery tactics are some combination of retrying an operation or maintaining redundant data or computations. Prevention tactics depend either on removing elements from service or utilizing mechanisms to limit the scope of faults.

可用性策略可分为故障检测，故障恢复和故障预防。 检测策略本质上取决于从各种组件中检测生命迹象。 恢复策略是重试操作或维护冗余数据或计算的某种组合。 预防策略取决于从服务中删除元素或利用机制来限制故障范围。

All of the availability tactics involve the coordination model because the coordination model must be aware of faults that occur to generate an appropriate response.

所有的可用性策略都涉及协调模型，因为协调模型必须知道发生的故障以生成适当的响应。

### 5.5 For Further Reading 进一步阅读

Patterns for availability:

* You can find patterns for fault tolerance in [Hanmer 07]. 

Tactics for availability, overall:

* A more detailed discussion of some of the availability tactics in this chapter is given in [Scott 09]. This is the source of much of the material in this chapter.
* The Internet Engineering Task Force has promulgated a number of standards supporting availability tactics. These standards include non-stop forwarding [IETF 04], ping/echo ICMPv6 [IETF 06b], echo request/response), and MPLS (LSP Ping) networks [IETF 06a].

Tactics for availability, fault detection:

* The parameter fence tactic was first used (to our knowledge) in the Control Data Series computers of the late 1960s.
* Triple modular redundancy (TMR), part of the voting tactic, was developed in the early 1960s by Lyons [Lyons 62].
* The fault detection tactic of voting is based on the fundamental contributions to automata theory by Von Neumann, who demonstrated how systems having a prescribed reliability could be built from unreliable components [Von Neumann 56].

Tactics for availability, fault recovery:

* Standards-based realizations of active redundancy exist for protecting network links (i.e., facilities) at both the physical layer [Bellcore 99, Telcordia 00] and the network/link layer [IETF 05].
* Exception handlinghas been written about by [Powel Douglass 99]. Software can then use this information to mask the fault, usually by correcting the cause of the exception and retrying the operation.
* [Morelos-Zaragoza 06] and [Schneier 96] have written about the comparison of state during resynchronization.
* Some examples of how a system can degrade through use (degradation) are given in [Nygard 07].
* [Utas 05] has written about escalating restart.
* Mountains of papers have been written about parameter typing, but [Utas 05] writes about it in the context of availability (as opposed to bug prevention, its usual context).
* Hardware engineers often use preparation-and-repair tactics. Examples include error detection and correction (EDAC) coding, forward error correction (FEC), and temporal redundancy. EDAC coding is typically used to protect control memory structures in high-availability distributed real-time embedded systems [Hamming 80]. Conversely, FEC coding is typically employed to recover from physical-layer errors occurring on external network links Morelos-Zaragoza 06]. Temporal redundancy involves sampling spatially redundant clock or data lines at time intervals that exceed the pulse width of any transient pulse to be tolerated, and then voting out any defects detected [Mavis 02].

Tactics for availability, fault prevention:

* Parnas and Madey have written about increasing an element’s competence set [Parnas 95].
* The ACID properties, important in the transactions tactic, were introduced by Gray in the 1970s and discussed in depth in [Gray 93].

Analysis:

* Fault tree analysis dates from the early 1960s, but the granddaddy of resources for it is the U.S. Nuclear Regulatory Commission’s “Fault Tree Handbook,” published in 1981 [Vesely 81]. NASA’s 2002 “Fault Tree Handbook with Aerospace Applications” [Vesely 02] is an updated comprehensive primer of the NRC handbook, and the source for the notation used in this chapter. Both are available online as downloadable PDF files.

### 5.6 Discussion Questions 问题讨论

1. Write a set of concrete scenarios for availability using each of the possible responses in the general scenario.
2. Write a concrete availability scenario for the software for a (hypothetical) pilotless passenger aircraft.
3. Write a concrete availability scenario for a program like Microsoft Word.
4. Redundancy is often cited as a key strategy for achieving high availability. Look at the tactics presented in this chapter and decide how many of them exploit some form of redundancy and how many do not.
5. How does availability trade off against modifiability? How would you make a change to a system that is required to have “24/7” availability (no scheduled or unscheduled downtime, ever)?
6. Create a fault tree for an automatic teller machine. Include faults dealing with hardware component failure, communications failure, software failure, running out of supplies, user errors, and security attacks. How would you modify your automatic teller machine design to accommodate these faults?
7. Consider the fault detection tactics (ping/echo, heartbeat, system monitor, voting, and exception detection). What are the performance implications of using these tactics?

